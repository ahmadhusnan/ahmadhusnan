OPENSHIFT
=========
COMPONENT BASED ON KUBERNETES
	kube-proxy			= maintain network between kubernetes resources
	kube-controller-manager		= Governs the state of the cluster
	kube-scheduler			= Allocate pods
	etcd				= Stores cluster data
	kube-apiserver			= Validates and configures data for the API objects
	kubelet				= Read container manifest (makesure container started and running)
	kubectl				= command to interact with kube-apiserver
	container runtime		= install podman / docker
	container-registry		= Stores and accessed container images
	pod				= smallest logical unit in kubernetes, contain one or more containers
	kubeconfig			= contains cluster, users, and context

SCALLING
	HORIZONTAL 			= deploy more instance
	VERTICAL			= Upgrade spek

FLOW
USER >>> Controller node (API server) >>> Worker Node {1..3} (Kubelet+Pods)
Contoller node			Compute Node			Storage/Registy
kube-apiserver			kubelet				PV storage
kube-scheduler			kube-proxy			Container registry
kube-controller-manager		CONTAINER RUNTIME
					Container
					Pods
----------------------------------------------------------------------------------
Physical			Virtual				Cloud

KUBERNETES RESOURCES
	Service				= expose a running application on a set of pods
	ReplicaSets			= maintain the constant pod number
	Deployment			= maintain the life cycle of an application

KUBERNETES VS OPENSHIFT
Kubernetes					OpenShift
Namespaces					Projects
----------					---------
Provides a scoping mechanism for resources.	Supports increased permissions control, creation requests, 
						and default templates for improved multitenancy workflows.
						Adds additional display name and description fields.
Ingresses 					Routes
---------					------
In a plain Kubernetes cluster, using ingress	Supports TLS termination and re-encryption.
resources requires installation of an ingress 	Supports HAProxy annotation for extended features,
controller. Different ingress controllers will 	such as enforcing HTTPS, selecting a load balancing algorithm,
provide different features.			and enabling rate limiting.
Deployments 					DeploymentConfigs
-----------					-----------------
Emphasizes availability over consistency.	Emphasizes consistency over availability.
Uses ReplicaSets that support set-based 	Supports automatic rollbacks.
match selectors.				Changes to the pod template automatically trigger rollouts.
Supports pausing rollouts.			Supports custom deployment strategies and lifecycle hooks.
Red Hat recommends using Deployments unless 
you need a specific DeploymentConfigs feature.
Kustomize 					Templates
---------					---------
Kubernetes kustomize manages configurations 	OpenShift Template resources are parameterized specification resources.
using file overlays without templates.		Create resources from templates using the oc process command or the web console.
Typically, manifests and kustomize overlays 
are stored in version control.

CLI
INSTALL OPENSHIFT CLIENT IN DESKTOP
	1. Download openshift client from redhat
		download
		- curl -LO "https://dl.k8s.io/release/$(curl -L \
  			-s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
		checksum
		- curl -LO "https://dl.k8s.io/$(curl -L \
			-s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
		check
		- echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
		install
		- sudo install -o root -g root -m 0755 kubectl \
  			/usr/local/bin/kubectl
	2. tar xvzf <FILE>
	3. echo $PATH
	4. mv <FILE> to $PATH directory
	
LOGIN
	LOGIN OCP only
	1. oc login -u user1
	LOGIN TO PROJECT
	2. oc login <optional HTTP/HTTPS/No proxy> -u user1
	3. oc cluster-info
	4. oc api-versions
	5. oc get clusteroperator
	6. oc config get-contexts
	7. oc config use-context
	8. oc config set-context <name-context> <*--namespace=namespace*>
	
PROJECT
	1. oc new-project <PROJECT NAME>
	2. oc projects
	3. oc status
	
POLICY
	1. oc adm policy add-role-to-user <ROLE\Name> <NAMA_USER/IDUSER> -n <NAMESPACE>
	   for f in nama/id_user nama/id_user nama/id_user; do oc adm policy \
	   add-role-to-user admin $f -n <NAMESPACE>; done;
	2. oc get rolebindings -owide -n <NAMESPACE> | grep nama/id_usr
	
APPLICATION
	1. Run app
	- oc new-app <IMAGE LINK/CONTAINER LINK>
	- oc create route edge <NAMA ROUTE> --service=<NAME SERVICE>
	- oc get all	
	2. Security Context Constrainys (SCCs)
	- Security mechanism that limits the access from a running pod in OpenShift to the host environment. 
	  SCCs control the following host resources:
		Running privileged containers
		Requesting extra capabilities for a container
		Using host directories as volumes
		Changing the SELinux context of a container
		Changing the user ID
	- command
		oc get scc (anyuid, hostaccess, hostmount-anyuid, hostnetwork, node-exporter, nonroot, previledged, restricted)
		oc describe scc
		oc get pod podname -o yaml | oc adm policy scc-subject-review -f -
		oc create serviceaccount <service account name> -n <namespaced>
		oc adm policy add-scc-to-user SCC -z <service-account name>
		change config
		oc set serviceaccount deployment/deployment-name <service-account-name>
		note*
		Restricted :
		mostly pod created vy openshift is restricted, 
		can't download images from public registry
		pod run using random userid, cannot run pod on previledged port (<1024)
		Anyuid :
		run as user to be RunAsAny = pod run as any available user ID
		
OPERATOR (cluster-admin)
	1. CLUSTER VERSION OPERATOR (CVO)
	- oc get clusteroperator / oc get co
	- oc describe clusteroperator <OPERATOR NAME>
	- oc explain
	2. OPERATOR LIFECYCLE MANAGER (OLM) and OperatorHub
	- The operator pattern is a way to implement reusable software to manage such complex workloads.
	- An operator typically defines custom resources (CRs). 
	- installed in openshift-operator
	3. Deploying Operator (Dashboard/Catalog)
	- Cluster operators
	  Cluster operators provide the platform services of OpenShift, such as the web console and the OAuth server.
	- Add-on operators
	  OpenShift includes the Operator Lifecycle Manager (OLM). 
	  The OLM helps users to install and update operators in a cluster. 
	  Operators that the OLM manages are also known as add-on operators, 
	  in contrast with cluster operators that implement platform services.
	- Other operators
	  Software providers can create software that follows the operator pattern, and then distribute the software as manifests, Helm charts, or any other software distribution mechanism.
	4. Implementing Operator
	- Uses Kubernetes API to watch instances of the CRs and to create matching workloads
		- Operator SDK 		= Go programming language and ansible, tools to package Helm Charts as operator
		- Java Operator SDK	= Java programming language, quarkus extentsion to defelop operator
	5. Operator details
		- Details
		  Displays information about the CSV.
		- YAML
		  Displays the CSV in YAML format.
		- Subscription
		  In this tab, you can change installation options, such as the update channel and update approval.
		  This tab also links to the install plans of the operator. 
		  When you configure an operator for manual updates, you approve install plans for updates in this tab.
		- Events
		  Lists events that are related to the operator. 
	6. Troubleshooting Operators
		- examine status and condition of the CSV, subscription and install plan resources
	7. Install operator in CLI
		- Locate the operator to install.
		- Review the operator and its documentation for installation options and requirements.
		- Decide the update channel to use.
		- Decide the installation mode. For most operators, you should make them available to all namespaces.
		- Decide to deploy the operator workload to an existing namespace or to a new namespace.
		- Decide whether the Operator Lifecycle Manager (OLM) applies updates automatically,
		  or requires an administrator to approve updates.
		- Create an operator group if needed for the installation mode.
		- Create a namespace for the operator workload if needed.
		- Create the operator subscription.
		- Review and test the operator installation.
	8. Delete Operator
		- oc delete sub <subscription name>
		- oc delete csv <currentCSV>
	9. OLM Resources
		- Catalog source
		Each catalog source resource references an operator repository. 
		Periodically, the OLM examines the catalog sources in the cluster and retrieves information about the operators in each source.
			oc get catalogsource -n openshift-marketplace
			oc get csv -A
		- Package manifest
		The OLM creates a package manifest for each available operator. 
		The package manifest contains the required information to install an operator, such as the available channels.
			oc get packagemanifests
			oc describe packagemanifest web-terminal -n openshift-marketplace
		- Operator group
		Operator groups define how the OLM presents operators across namespaces.
			---
			apiVersion: operators.coreos.com/v1
			kind: OperatorGroup
			metadata:
			  name: name
			  namespace: namespace 1
			spec:
			  targetNamespaces: 2
			  - namespace
		- Subscription
		Cluster administrators create subscriptions to install operators.
			---
			apiVersion: operators.coreos.com/v1alpha1
			kind: Subscription
			metadata:
			  name: web-terminal
			  namespace: openshift-operators 1
			spec:
			  channel: fast 2
			  name: web-terminal 3
			  source: do280-catalog-redhat 4
			  installPlanApproval: Manual 5
			  sourceNamespace: openshift-marketplace
		- Operator
		The OLM creates operator resources to store information about installed operators.
		- Install plan
		The OLM creates install plan resources as part of the installation and update process. 
		When requiring approvals, administrators must approve install plans.
			oc patch installplan install-pmh78 --type merge -p '{"spec":{"approved":true}}' -n openshift-file-integrity
		- Cluster service version (CSV)
		Each version of an operator has a corresponding CSV. 
		The CSV contains the information that the OLM requires to install the operator.
		- Updates Operator
			- OpenShift provides features to help to implement such policies.
			For each installed operator, you can decide whether the OLM automatically applies updates, or whether the updates require administrator approval.
			- Operator providers can create multiple channels for an operator. 
			The provider can follow different policies to push updates to each channel, so that each channel contains different versions of the operator. 
			When installing an operator, you choose the channel to follow for updates.
			- You can create custom catalogs, and decide which versions of operators to include in the catalog. 
			For example, in a multicluster environment, you configure operators to update automatically, but add only tested versions to the catalog.
	
VIEW PODS
	1. oc get pods -o wide -A (all namespaces)
		i. oc get pod -A --sort-by='{.metadata.creationTimestamp}' --show-labels
	2. oc logs <POD NAME>
	3. oc describe pods <POD NAME>
	4. oc get service
	5. oc get pods <PODS NAME> -o yaml
	   ex:
	   oc get pods -o yaml | yq r - 'items[0].status.podIP'  <<<< filter [index in the item array]
	   oc get pods \
	   -o custom-columns=PodName:".metadata.name",\
	   ContainerName:"spec.containers[].name",\
	   Phase:"status.phase",\
	   IP:"status.podIP",\
	   Ports:"spec.containers[].ports[].containerPort"
	6. oc explain
	7. oc adm top pods -A --sum
	8. oc adm top pods <POD NAME> -n <NAMESPACE> --containers
	9. oc logs pod-name -c <CONTAINER NAME>
	10. oc debug
	11. oc debug job/<JOB NAME> --as-user=100000
RUN PODS
	1. oc run <NAME POD> --image <IMAGENAME> --it(interaktif) --command -- <COMMAND EXECUTED> -- restart <RESTART OPTION> -- env <ENV VALUES>
	ex:
	oc run mysql --image registry.redhat.io/rhel9/mysql-80 --env MYSQL_ROOT_PASSWORD=myP@$$123
	2. oc exec <PODNAME> <OPTION> -- <COMMAND>
	ex:
	oc exec my-app -c ruby-container -- date
	oc exec my app -c ruby-container -it -- bash -il
LOGS POD
	oc logs <POD NAME> -l/--selector='' --tail= -c/--container= -f/--follow
	
DELETE PODS
	1. oc delete pod <NAME POD> -n <NAMESPACE> -l <LABEL> --grace-period=
	2. ns=<NAMESPACE>; for i in $(oc get pod -n $ns --no-header) | grep -i completed | awk '{print $1}'); do oc delete pod $i -n $ns;done
	
CLUSTER EVENTS and ALERTS
	EVENTS
	1. oc get events -n <NAMESPACE>
	2. oc describe pod <POD NAME>
	ALERTS (openshift-monitoring namespace)
	1. oc get all -n <NAMESPACE> --show-kind
	2. oc logs <POD ALERT>

CLUSTER NODE STATUS
	1. oc cluster-info
	2. oc get nodes
	   ex:
	   oc get node master01 -o json | jq '.status.conditions'
	3. oc adm node-logs --role <ROLE NAME> -u <SPECIFIED UNIT> --path=<COMMAND> --tail <BARIS>
	   ex:
	   oc adm node-logs --role master -u kubelet --path=cron --tail 1
	4. oc debug node/<NODE NAME> <<<< accest via kubelet
	5. chroot /host
	6. oc adm must-gather --dest-dir </PATH>
		tar cvaf </PATH/file must gather>
	7. oc adm inspect <SPECIFIED RESOURCES> --since <LAST TIME>
	NODE CONDITION
	CONDITION			DESCRIPTION
	OutOfDisk			If true, has insufficient free space for adding new pods
	NetworkUnavailable		If true, network not correctly configured
	NotReady			If true, one of components, such as container runtime or network is experiencing issue or not configured
	SchedullingDisabled		If true, Pods cannot be scheduled for placement on the node
	
	
SCALING THE APPLICATION
	1. oc scale --current-replicas=1 --replicas=2 deployment/<DEPLOY NAME>
	
	DEPLOY BACK-END SERVICE(Python)
		1. oc new-app python~https://github.com/openshift-roadshow/nationalparks-py.git --name nationalparks -l 'app=national-parks-app,component=nationalparks,role=backend,app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=python' --allow-missing-images=true
	CONNECTING TO A DATABASE
		1. oc new-app quay.io/centos7/mongodb-36-centos7 --name mongodb-nationalparks -e MONGODB_USER=mongodb -e MONGODB_PASSWORD=mongodb -e MONGODB_DATABASE=mongodb -e MONGODB_ADMIN_PASSWORD=mongodb -l 'app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=mongodb'
	SECRET
		1. oc create secret generic nationalparks-mongodb-parameters --from-literal=DATABASE_SERVICE_NAME=mongodb-nationalparks --from-literal=MONGODB_USER=mongodb --from-literal=MONGODB_PASSWORD=mongodb --from-literal=MONGODB_DATABASE=mongodb --from-literal=MONGODB_ADMIN_PASSWORD=mongodb
		2. oc set env --from=secret/nationalparks-mongodb-parameters deploy/nationalparks
		3. oc rollout status deployment nationalparks
		4. oc rollout status deployment mongodb-nationalparks
		
	DEPLOY MYSQL
		1. oc new-app mysql MYSQL_USER=user1 MYSQL_PASSWORD=mypa55 MYSQL_DATABASE=testdb -l db=mysql
	DEPLOY From Docker
		1. oc new-app --docker-image=docker.io/indr01/nama_image:tag --name=nama_app
	DEPLOY From Github
		1. oc new-app https://github.com/openshift/ruby-hello-world --name=ruby-hello
		
ANY UID (SERVICE ACCOUNT)
	sa = service account
	scc = security context Contraints
	1. oc create sa anyuid-nama_pod 
	     ex: oc create sa anyuid-httpd
	   oc adm policy add-scc-to-user <ANYUID> -t <ANYUID-NAME_POD>
	     ex: oc adm policy add-scc-to-user anyuid -z httpd
Perintah di atas akan memberikan role "AnyUID" kepada pod dengan nama "my-pod". Anda juga dapat menggunakan perintah oc adm policy add-scc-to-group untuk memberikan role "AnyUID" kepada sekelompok pod.

oc adm set sa dc/httpd anyuid-httpd
oc logs nama_pods // melihat logs
contoh : oc logs httpd-2-cbs29

API-RESOURCES
	1. spec 	= desired status
	2. status 	= current status
	3. command
		oc api-resources

IMAGE STREAM
	1. Advantages
		- Tagging images without pushing using the command line.
    		- Configuring Builds and Deployments for automatic redeployment upon image updates.
		- Fine-grained access control and image sharing across teams.
		- Improving security by configuring view and use permissions on the image stream object.
		- Avoiding application downtime that can occur when using untested image versions.
		- Users without permissions on the source images can interact with image streams.
	2. Annotating (JSON image.openshift.io/triggers)
		- oc set trigger deploy/<DEPLOYMENT_NAME> --from-image <IMAGE_STREAM_TAG> -c <CONTAINER_NAME>.
IMPORT-IMAGE
	1. oc import-image my-ruby
	--check images update periodically
	2. oc import-image <Image> --confirm --scheduled
BUILD 
	1. oc new build </GIT_DIRECTORY>
		from build config
		1. oc start-build python

DEPLOY
	Template (YAML manifest in openshift namespace)
		1. oc process -f <templateYAML.yaml> -o yaml 
			1.1. oc process -f <templateYAML.yaml> --parameters
		2. oc get template -n openshift
		3. oc process --parameters <template name> -n openshift
		3. oc new-app --template=<template name>
		4. oc porcess -f <template name> > <name manifest.yaml>
		5. oc process <name yaml> --param-file=<param.env> > <manifest.yaml>
		6. oc process <name> --param-file=<param.env> | oc apply -f .
		7. oc create -f (update template to specific project)
		
	CREATE DEPLOY
		1. kubectl apply -f <FILE/FILEADDR/ADDR.yaml>
			REQ FIELD
			1. apiVersion			= v1/v2/v3
			2. kind				= Deployment
			3. metadata
				1. name
				2. UID
				3. namespace
			4. spec
		2. kubectl get deployments
			filtering using filter
				oc get deployment -n openshift-cluster-storage-operator --show-labels
				oc get deployment -n openshift-cluster-storage-operator -l app=csi-snapshot-controller-operator -o name
		3. oc new-app --template <TEMPLATE_NAME>
			3.1. oc new-app --template <TEMPLATE_NAME> --param <ENV=PARAMETER> --image <IMAGE>
			3.2. oc new-app <REPONAME>
			filtering jsonpath
				iterate list [*]= oc get deployment -n openshift-cluster-samples-operator cluster-samples-operator -o jsonpath='{.status.conditions[*].type}'
				index notation [0]
				condition	= jsonpath='{.status.conditions[?(@.type=="Available")].status}'
				range iteration = oc get pods -A -o jsonpath='{range .items[*]}' '{.metadata.namespace} {.metadata.creationTimestamp}{"\n"}'
			create jsonpath for report
				oc get nodes -o jsonpath-file=not_ready_nodes.jsonpath
				---jsonpath
				{range .items[*]}
				  {.metadata.name}
				  {range .status.conditions[?(@.status=="False")]}
				    {.type}{"="}{.status} {.message}
				  {end}
				{end}
		4. oc create deployment <DEPLOY_NAME> --image <IMAGE>
		5. oc run <POD NAME> --image=<IMAGE> --env <Env=PARAMETER> --port <PORT> 
		6. Waiting on Triggered Deployment Updates
		   - update secret
			deployment_generation=$(oc get -n openshift-authentication deployment/oauth-openshift -o jsonpath='{.status.observedGeneration}')
			- apply change
			while
			    new_generation=$(oc get \
			        -n openshift-authentication deployment/oauth-openshift \
			        -o jsonpath='{.status.observedGeneration}')
			    [ $new_generation -eq $deployment_generation ]
			do
		    		sleep 3
			done
			oc rollout status -n openshift-authentication deployment/oauth-openshift --revision=$new_generation
			while [ -n "$(oc get pod -n openshift-authentication -o \
			jsonpath='{.items[?(@.metadata.deletionTimestamp != "")].metadata.name}')" ]
			do
			    sleep 3
			done
	HELM CHART
	- Package that describes a set of Kubernetes resources that you can deploy. 
	- Helm charts define values that you can customize when deploying an application. 
	- A chart is a collection of files with a defined structure
	- instead of specifying the image for a deployment, charts can use user-provided values for the image. 
	- Helm charts can contain hooks that Helm executes at different points during installations and upgrades. 
	- Hooks can automate tasks for installations and upgrades. 
	- With hooks, Helm charts can manage more complex applications than purely manifest-based processes.
	1. Structure (Chart, Release, Versions)
		sample/
		├── Chart.yaml 1
		├── templates 2
		|   |── example.yaml
		└── values.yaml 3
	2. Command
		helm show chart <chartname>
		helm show values <chartname>
	3. Installing
		helm install <release-name> <chartreference>  --dry-run  --values values.yaml
		*note
			To install a chart, you must decide on the following parameters:
				The deployment target namespace
				The values to override
				The release name
	4. Release
		helm list --all-namespaces or -n 
	5. Upgrade
		helm upgrade
	6. Rollback
		helm history <relese name>
		helm rollback <releasename> <revision>
	7. Helm Repositories ( ~/.config/helm/repositories.yaml)
		helm repo add <name chart> <https://url.chart/>
		helm search repo

	ROLLOUT
		1. kubectl rollout status deployment/<DEPLOY_NAME>
	REPLICASET
		1. kubectl get rs
			ex : [deployment]-[name]-[hash]
		   	      kubectl get rs nginx-deployment-979797
	LABEL
		1. kubectl get pods --show-labels
	UPDATE DEPLOYMENT
		1. kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
		2. oc set env deployment/<DEPLOY_NAME> <env=<PARAMETER>>
	DESCRIBE DEPLOYMENT
		1. kubectl describe deployments
		2. oc describe template mysql-epemeral -n openshift
	SCALING
		1. kubectl scale deployment/<DEPLOY_NAME> --replicas=10 
		if horizontal pos scalling enabled
			kubectl scale deployment/<DEPLOY_NAME> --min=10 --max=15 --cpu-percent=80
			oc scale --replicas 5 deployment/scale
	
TAG
	1. oc tag ruby:latest ruby:2.0

RESOURCE
	Types:
	1. pod (Pods)
	2. svc (Services)
	3. rs (ReplicaSet)
	4. pv (Persitent Volumes)
	5. pvc (Persistent Volume Claims)
	6. cm (ConfigMaps) and Secrets
	7. deploy (Deployment)
	8. bc (BuildConfig)
	9. dc (DeploymentConfig)
	10. routes
	11. scc (Security Context Constrains)
	12. sa (Service Account)
	every resource has .kind .apiVersion .status
	ex:
		pod.kind pod.api.Version .status
	Command:
	1. oc api-resources
	ex:
		oc api-resources --namespaced=true --api-group apps --sort-by name
	Condition Resource fields
	Field			Example				Description
	Type			ContainerReady			The Type of the condition
	Status			False				The state of the condition
	Reason			RequirementsNotMet		An optional field to provide extra info
	Message			2/3 container are running	An optional textual desc for condition
	LastTransitionTime	2023-03-07T18:05:28Z
	API Resource field
	Field			Example				Desc
	apiVersion		v1				Identifier of the object schema version
	kind			Pod				Schema Identifier
	metadata.name		<name>				Creates a label with a name key
	metadata.namespace	<namespace>			The namespace located
	metadata.labels		<label>	: app: group:		Key-value pairs identifieying metadata
	
APPLY CONFIG
	1. oc apply -f pod.json
	2. oc create -f pod.json
	3. oc delete pod/<PODNAME>
	4. oc delete pods -l app=parksmap-katacoda
	
AUTOSCALE
	1. oc autoscale deploymentconfig/<DC_NAME> --min=2 --max=5

ROLLBACK
	1. oc rollback php (--to-version=3)
	
ROLLOUT
	1. oc rollout undo deploymentconfig/<DC_NAME>
	
EDIT
	1. oc edit deploymentconfig/<PODNAME>
	2. OC_EDITOR="nano" oc edit deploymentconfig/<PODNAME>
	3. oc edit deploymentconfig/<PODNAME> -o json
	
EXPOSE (Make deployment to create service)
	1. oc expose service/<PODNAME>
	2. oc expose service/<PODNAME> --hostname=www.my-host.com
	
GET
	1. oc get pods -n default
	2. oc get nodes
	3. oc get deploymentconfig/python -o json
	
HELP
	1. oc help 
	or
	oc create --help
	2. oc explain pods
	
LOGOUT
	1. oc logout

CONTAINER IMAGES
	1. ENV
		- define the available env variable 
	2. ARG
		- define build-time variables
	3. USER
		- define the active user in container (to prevent using root)
	4. ENTRYPOINT
		- define the executeable to run when container started
	5. CMD
		- define command when start based on ENTRYPOINT
	6. WORKDIR
		- set of current working dir within container
	7. METADATA
	   EXPOSE
		- define where to store data outside container
	   VOLUME
		- where to store data outside the container
	   LABEL
		- add a key-value pair

TROUBLESHOOTING TOOLS OCP
	1. kubectl describe 	: describe resources
	2. kubectl edit 	: edit config resources
	3. kubectl patch	: update specific attribute or field for a resources
	4. kubectl replace	: deploy a new instance of the resource
	5. kubectl cp		: copy from or to container
	6. kubectl exec		: execute command within a specified container
	7. oc status		: display status container
	8. oc explain		: display documentation
	9. oc rsync		: syncronize files and dir
	10. oc rsh		: start remote shell within a specified container
	11. oc port-forward	: configure port forwarder
	12. oc logs		: retrieve logs for specified container

LONG-LIVED or SHORT LIVED APP
	JOBS (one0time task)
	1. oc create job <JOB-NAME> --image <IMAGE> -- /bin/bash -c "date"
	2. oc create cronjob <CRON-NAME> --image <IMAGE> --schedule="* * * * *" -- <COMMAND/date>
	3. oc create deployment <DEPLOYMENT-NAME> --image <IMAGE> --replicas
	DEPLOYMENT
	STATEFUL SETS

POD and SERVICE NETWORK
	KUBERNETES NETWORKING
		1. Highly containet-to-container communications
		2. Pod-to-pod communications
		3. Pod-to-service commnunication
		4. External-to-service communication
	Create SVC for Deployment
	1. oc expose deployment/<DEPLOY-NAME> [--selector <SELECTOR>] [--port <PORT>] [--target-port <TARGET PORT>] [--protocol <PROTOCOL>] [--name <NAME>]
	Chcek SVC
	1. oc get service <SVC-NAME> -o wide
	2. oc get endpoints
	3. oc describe deployment === finde Selector
	KUBERNETES DNS for SERVICE DISCOVERY
		Process
		1. Using DNS Operator > Deploy CoreDNS > Create svc resource > kubelet instruct pods to use CoreDNS service IP
		Each service dynamicaly assigned by Fully Qualified Domain Name (FQDN)
		SVC-NAME.PROJECT-NAME.svc.CLUSTER-DOMAIN
		SVC-NAME.PROJECT-NAME
		SVC-NAME
		Check from the container within service
		1. cat /etc/resolve.conf
	KUBERNETES NETWORKING DRIVERS
		Container Network Interface Plug-ins
		1. OVN-Kubernetes (RHOCP 4.10)
			1. oc get -n openshift-network-operator deployment/network-operator
			2. oc describe network.config/cluster
		2. Openshift SDN (RHOCP 3.x)
		3. Kuryr 
	SCALE and EXPOSE APPLICATIONS
	Service Types
		1. Cluster IP	: expose svc on cluster internal IP
		2. Load Balancer: instruct kubernetes to interact with cloud provider, then provide externally accesible IP to APP
		3. ExternalIP	: redirect traffic from a virtual IP addresses on a cluster node to pod. Cluster admin assign virtual IP to a a node, instruct RHOCP to set NAT rules
		4. NodePort	: expose svc on a port on the node IP address, redirect endpoints(pods) of the service
		5. ExternalName	: tell kubernetes that the DNS name in the externalName back the service, DNS req against Kubernetes DNS, it returns the externalName in Cannonical Name((CNAME)
	Using Routes
		Process
		1. expose HTTP and HTTPS trafic, TCP app, and non-TCP trafic by only expose HTTP and TLS-based app 
		External Request >> Ingress >> Pod >> Routes >> Service
		Resources
		1. Routes and Ingress traffic >> expose app to external network << Traffic convert by ingress from external to pods
		Create
		1. oc expose service <service name> --hostname <hostname>
		frontend-api.apps.example.com  << <ROUTE-NAME>-<PROJECTNAME>.<DEFAULT-DOMAIN>
		2. oc delete route myapp-route
		Ingress
		1.  oc create ingress ingr-sakila --rule="ingr-sakila.apps.ocp4.example.com/*=sakila-service:8080"
		 oc create ingress ingress-name --rule=URL_route=service-name:port-number
		2. oc delete ingress example-ingress
	Sticky Session (cookie)
		RHOCP auto generates the cookie for an ingress object
		create manual
		1. oc annotate ingress <INGR-NAME> ingress.kubernetes.io/affinity=cookie
		2. oc annotate route <INGR-NAME> router.openshift.io/cookie_name=myapp
		capture hostname
		3. ROUTE_NAME=$(oc get route <ROUTE_NAME> -o jsonpath='{.spec.host}')
		save and access routes
		4. curl $ROUTE_NAME -k -c /tmp/cookie_jar
		to connect again
		5. curl $ROUTE_NAME -k -b /tmp/cookie_jar

	MANAGE STORAGE for APP CONFIG and DATA
		1. CONFIGMAP
		provide ways to inject config data into containers (Not need protection)
		2. SECRETS
		store sensitive information (password, sesitive config files, cred, ssh key, auth token, tls certificate)
		encoded Base64
		not encrypted
		ex: echo bXl1c2VyCg== | base64 --decode
			2.1.  oc create secret generic <SECRET_NAME> --from-literal <KEY1=<SECRET1>> --from-literal <KEY2=<SECRET2>>
			2.2. kubectl create secret generic ssh-keys --from-file id_rsa=/path-to/id_rsa --from-file id_rsa.pub=/path-to/id_rsa.pub
			2.3. kubectl create configmap <CONFIG_NAME> --from-literal <key1=<config1>> --from-literal <key2=<config2>>
			2.4. oc create secret generic <SECRET_NAME> --from-file user=/tmp/demo/user --from-file root_password=/tmp/demo/root_password
			2.5 oc set volume deployment/demo --add --type secret <TYPE_SECRET> --secret-name <SECRET_NAME> --mount-path </MOUNTDIR> 
			2.6  oc delete configmap/<CONFIGMAP_NAME> -n <NAMESPACES>
	Block Storage for Databases (iSCSI & Fibre Channel)
	1. Benefits :
		- speed
		- efficiency
	2. Drawback :
		- Block storage is not a shared read-write storage technology. Only one host at a time can write data.
		- Block storage can be expensive.
		- Block storage does not provide robust metadata about stored data. 
		  Adding a file system to a block device provides additional metadata for each record, such as a file name or permissions.
	3. Schema :
		- data record to multiple block
	4. Accessing
		- app needs direct access to the block storage device
		---
		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: fc-block-pv-01 1
		spec:
  		accessModes:
  		  - ReadWriteOnce
		  capacity:
 		  storage: 10Gi
		  storageClassName: fc-block 2
		  volumeMode: Block 3
		  fc: 4
		    targetWWNs: ["500...fd1"]
		    lun: 0
		    readOnly: false
		---
		apiVersion: v1
		kind: PersistentVolumeClaim
		metadata:
		  name: block-pvc
		spec:
		  accessModes:
		    - ReadWriteOnce
		  volumeMode: Block 1
		  resources:
		    requests:
		      storage: 10Gi
		  storageClassName: fc-block 2
		---
		apiVersion: v1
		kind: Pod
		metadata:
		  name: pod-with-block-volume
		spec:
		  containers:
		    - name: fc-container
		      image: fedora:26
		...output omitted...
		      volumeDevices: 1
		        - name: data 2
		          devicePath: /dev/xvda 3
		  volumes:
		    - name: data 4
		      persistentVolumeClaim:
		        claimName: block-pvc 5
	5. Binding Scenarios
	PVC Volume Mode		PV Volume Mode	Result
	Unspecified		Block 		Does not bind.
	Filesystem 		Block 		Does not bind.
	Block 			Block 		The PV binds to the PVC.
	
	Unspecified		Unspecified	The PV binds to the PVC, but the PV is not a block volume. Unspecified values default to Filesystem.
	Persistent Data Volumes
		1. Method :
		- Static 	: create PV manual for cluster
		- Dynamic	: uses storage classes to create the PV on demand
		2. Storage Volume types :
		- configMap 	: app configuration in external places
		- emptyDir	: per-pod directory for scratch data
		- hostPath	: volume mount from host node to pod (must run as previlaged)
		- iSCSI		: Internet Small Computer System Interface is IP based provide block-level access to storage devices
		- local		
		- NFS		: can accessed by multiple pod at same times
		3. Persistent Volume (PV)
		---
		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: pv0003 1
		spec:
		  capacity:
		    storage: 5Gi 2
		  volumeMode: Filesystem 3
		  accessModes: 4
		    - ReadWriteOnce
		  persistentVolumeReclaimPolicy: Recycle 5
		  storageClassName: slow 6
		  mountOptions:
		    - hard
		    - nfsvers=4.1
		  nfs: 7
		    path: /tmp
		    server: 172.17.0.2
		4. Persistent Volume Claims(PVC)
		- tied with namespaced not pods
		- declare what an app needs, which Kubernetes porvides on a best-effort basis
		- Specified : Storage size, Storage class, access mode
			Create PVC > update in deployment
			3.1. oc set volumes deployment/<DEPLOY_NAME> \ 1
				--add \ 2
				--name <NAME_PV> \ 3
				--type <persistentVolumeClaim> \ 4
				--claim-mode <rwo> \ 5
				--claim-size <15Gi> \ 6
				--mount-path </var/lib/example-app> \ 7
				--claim-name <example-pv-claim> 
			3.2. YAML (kind: PersistentVolumeClaim)
		5. Storage Class (Policy set)
		Manual Reclaim
			4.1 oc delete pv <PV_NAME>
			4.2 oc create pv <using data from previous PV>
			4.3 Remove data on the storage asset, and delete the storage asset
		6. Network-Attached Storage(NAS)
		File based storage architecture using two protocol IP or TCP
		- single access point
		- built in security 
		- fault-tolerance
		6. Stateful Sets87
		7. Storage Operator
		- manage the provisioning of persistent volumes and related storage resources.
			oc get clusterserviceversions --all-namespaces
			oc create -f local-volumes.yml -n local-storage
		---
		apiVersion: "local.storage.openshift.io/v1"  1
		kind: "LocalVolume"  2
		metadata:
		  name: "local-disks"  3
		spec:
		  storageClassDevices:
		    - storageClassName: "fast-local-storage"  4
		      volumeMode: Filesystem
		      devicePaths:
		        - /dev/sdb  5
		    - storageClassName: "standard-local-storage"  6
		      volumeMode: Filesystem
		      devicePaths:
		        - /dev/hda  7
		8. Ansible Playbooks
		- Use a playbook to create and manage storage volumes, persistent volumes, and storage classes.
		9 Shared Storage
		- Benefits :
			CI/CD
			Artificial Intelgence Workload
			Streaming Application
		- Types :
			File Storage	: Network File System (NFS), Server Message Block (SMB), Common Internet File System (CIFS)
			Block Storage	: Storage Area Network (SAN), Amazon Elastic Block Store (EBS), Google Cloud Persistent Disk
			Object Sorage(arbitrary metadata) : Amazon Simple Storage Service (S3), Red Hat Ceph Storage, Openstack Object Storage (Swift)
		10. Azure File-System
		    ---
			apiVersion: storage.k8s.io/v1
			kind: StorageClass
			metadata:
			  name: azurefile-lrs 1
			provisioner: kubernetes.io/azure-file 2
			parameters:
			  skuName: Standard_LRS 3
			  storageAccount: azure_storage_account_name 4
			  secretNamespace: my-secret-namespace 5
			  secretName: azure-storage-credentials 6
		11. Default Storage Class
			oc annotate storageclass standard --overwrite "storageclass.kubernetes.io/is-default-class=true"
			cat set_default.yml
			oc patch storageclass standard -p "$(cat set_default.yml)"
			oc get storageclasses
			---
			apiVersion: storage.k8s.io/v1
			kind: StorageClass
			metadata:
  			annotations:
			    storageclass.kubernetes.io/is-default-class: "true" 1
		12. Restricting Storage Access
			Resource Name		Description
			requests.storage 	The sum of storage space requests across all persistent volume claims can not exceed this value.
			persistentvolumeclaims 	The total number of persistent volume claim resources that can exist in a project.
			<storage-class-name>.storageclass.storage.k8s.io/ requests.storage 	The sum of storage space requests for the across all persistent volume claims, but only for the <storage-class-name> storage class.
			<storage-class-name>.storageclass.storage.k8s.io/ persistentvolumeclaims 	The total number of persistent volume claim resources for the <storage-class-name> storage class that can exist in a project.			
			---
			apiVersion: v1
			kind: ResourceQuota
			metadata:
			  name: storage 1
			  namespace: test 2
			spec:
			  hard:
			    persistentvolumeclaims: 4 3
			    requests.storage: 100G 4
		13. Local Block Storage
			---
			apiVersion: v1
			kind: PersistentVolume
			metadata:
			  name: local-pv-01
			spec:
			  capacity:
			    storage: 500Gi
			  accessModes:
			  - ReadWriteOnce
			  persistentVolumeReclaimPolicy: Retain
			  storageClassName: local-storage
			  local:  1
			    path: /mnt/disks/vol1  2
			  nodeAffinity:
			    required:
			      nodeSelectorTerms:
			      - matchExpressions:
			        - key: kubernetes.io/hostname  3
			          operator: In
			          values:
			          - my-node

	HIGH AVAILABILITY APPLICATION
	1. Restart Pods (configuring restart policy on pod)
		1.1. Compute Resource Requests
		- Request Request 	: Specify minimum required compute resources necessary to schedule a pod
			oc set resources deployment <DEPLOY_NAME> --requests <cpu=10m,memory=1gi>
		- Inspecting Cluster Compute Resourcess
			oc describe node <NODE_NAME>
			oc adm top node <CLUSTER_NAME>
			oc adm top pods -n <NAMESPACE>
		- Memory Limit		: If achieved, the process in the container killed by triggered OOM killer 
			oc get pod <podname> -o yaml
		- CPU Limit		: if achieved, inhibit the container, slow down the pace(CPU pressured)
	2. Probes (Check when application cannot respond to request)
		2.1. Probe Endpoints
		Function :
		- Crash mitigation by automatically attempting to restart failing pods
		- Failover and load balancing by sending requesst only to healthy pods
		- Monitoring when pods are failing
		- Scalling by determining when a new replica is ready
		Probe Types :
		- Readines Probes	: determine requests to accept or prevent by removing IP
		- Liveness Probes	: called throughtout the lifetime of the application by restarting or recreatign the pod (restart policy)
		- Startup Probes
		Tests :
		- HTTP GET 		: Probe > req to specified HTTP endpoint (success if code 200 or 399)
		- Container Command 	: Command test > Oode 0 Success
		- TCP Socket		: Cluster Open Socket to container , if connection is established
		Parameter :
		- Timing
	3. Horizontal Scaling (When load change, replicas match the load)
		- Resources Types 	: HorizontalPod Autoscaler
		- Work Flow		: Retrieves the details metric for scalling HPA > Auto scaller collect metric from metric subsystem > computes the usage percentage > computes average accross all targeted pods > get ratio
			oc autoscale deployment/hello --min 1 --max 10 --cpu-percent 80
			oc get hpa

	PRODUCTION (multi tenancy and security, operator)
	Imperative 	= command based
	Declarative	= manifest based JSON & YAML
	1. Workflow
		kubectl create -f <manifest.yml/url yml> --recursive=true/-R --save-config
		kubectl apply -f <manifest.yml/url yml> *after --save-config
	2. YAML Validation (Testing)
		--dry-run=server	= submit server-side request without persisting the resources
		--validate=true		= validate the input
	3. Comparing Resources
		kubectl diff -f <manifest.yml>
	4. Update Consideration
		not every change restart the pod, secrets and configmaps only happen in startup
		oc delete pod <pod name>
		oc rollout restart deploymeng <deployment name>

	KUSTOMIZE
	Configuration management tool to make declarative changes to app configurations and components and oreserve the original YAML
	(kustomization.yaml at rooot)
	Work well with GiTOPS
	Patch customization
	1. Command
		- kubectl kustomize overlay/production
		- kubectl apply -k overlay/production
		- oc delete kuztomize overlay/production
	2. Structure Base directory
		base
		├── configmap.yaml
		├── deployment.yaml
		├── secret.yaml
		├── service.yaml
		├── route.yaml
		└── kustomization.yaml
			ex
			apiVersion: kustomize.config.k8s.io/v1beta1
			kind: Kustomization
			resources:
			- configmap.yaml
			- deployment.yaml
			- secret.yaml
			- service.yaml
			- route.yaml
	3. Overlays declarative artifacts, patches
		base
		----
		kustomization.yaml + resources
			development
			-----------
			kustomization.yaml (refers to base)
			testing
			-------
			kustomization.yaml (refers to base + pathces)
			prod
			----
			kustomization.yaml (refers to base + patch.yaml)
			ex.
			[user@host frontend-app]$ tree
			base
			├── configmap.yaml
			├── deployment.yaml
			├── secret.yaml
			├── service.yaml
			├── route.yaml
			└── kustomization.yaml
			overlay
			└── development
  			  └── kustomization.yaml
				apiVersion: kustomize.config.k8s.io/v1beta1
				kind: Kustomization
				namespace: dev-env
				bases:
				- ../../base
			└── testing
    			  └── kustomization.yaml
				apiVersion: kustomize.config.k8s.io/v1beta1
				kind: Kustomization
				namespace: test-env
				patches: 1
				- patch: |-
    					- op: replace 2
      					  path: /metadata/name
      					  value: frontend-test
  				  target: 3
    				    kind: Deployment
    				    name: frontend
				- patch: |- 4
    					- op: replace
      					path: /spec/replicas
      					value: 15
  				  target:
    					kind: Deployment
    					name: frontend
				bases: 5
				- ../../base
				commonLabels: 6
  				  env: test
			└── production
  			  ├── kustomization.yaml
  			  └── patch.yaml
				apiVersion: kustomize.config.k8s.io/v1beta1
				kind: Kustomization
				namespace: prod-env
				patches: 1
				- path: patch.yaml 2
				  target: 3
				    kind: Deployment
				    name: frontend
				  options:
				    allowNameChange: true 4
				bases: 5
				- ../../base
				commonLabels: 6
				  env: prod
					{patch.yaml reference}
					apiVersion: apps/v1
					kind: Deployment
					metadata:
					  name: frontend-prod 1
					spec:
					  replicas: 5 2
	4. Fitures
		- configMapGenerator
			configMapGenerator:
			- name: configmap-1  1
 			 files:
			    - application.properties
			- name: configmap-2  2
			  envs:
			    - configmap-2.env
			- name: configmap-3  3
			  literals:
			    - name="configmap-3"
			    - description="literal key-value pair"
		- secretGenerator
			secretGenerator:
			- name: secret-1  1
			  files:
			    - password.txt
			- name: secret-2  2
			  envs:
			    - secret-mysql.env
			- name: secret-3  3
			  literals:
			    - MYSQL_DB=mysql
			    - MYSQL_PASS=root
		- generatorOptions
			disableNameSuffixHash: true
  			labels:
 			   type: generated-disabled-suffix
			annotations:
 			   note: generated-disabled-suffix

	AUTHENTICATION AND AUTHORIZATION (HTPasswd & RBAC)
	1. Component:
		- user
		- Identity
		- Service Account
		- Group
		- Role
	2. Process :
		- Authentication (who is previlage)
		- Authorization (what is previlage) (Role-based Access Control)
	3. Auth Method :
		- OAuth access token
		- X.509 client certificates
		- REST API
			 must retrieve a bearer token from the OpenShift OAuth server, 
			 and then include this token as a header in requests to the API server.
	4. Auth Operator
		- runs an OAuth server grant access token to user but validate first
	5. Identity Providers
		- HTPasswd 	= validates user & password againts secret produced by command htpasswd
			Only a cluster administrator can change the data inside the HTPasswd secret. Regular users cannot change their own passwords.
			most production environments require a more powerful identity provider that integrates with the organization's identity management system.
				apiVersion: config.openshift.io/v1
				kind: OAuth
				metadata:
				  name: cluster
				spec:
				  identityProviders:
				  - name: my_htpasswd_provider 1
				    mappingMethod: claim 2
				    type: HTPasswd
				    htpasswd:
				      fileData:
				        name: htpasswd-secret 3
			command
				oc get oauth cluster -o yaml > oauth.yaml
				oc replace -f oauth.yaml
			htpasswd comand (install httpd-tools)
				htpasswd -c(create) -B -b(add/update) /tmp/htpasswd student redhat123
				htpasswd -D /tmp/htpasswd student *delete cred
				oc create secret generic htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config
				oc extract secret/htpasswd-secret -n openshift-config --to /tmp/ --confirm
				oc set data secret/htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config (redeploy openshift-authentication)
				watch oc get pods -n openshift-authentication
				oc get identities | grep manager
				oc delete identity my_htpasswd_provider:manager
				oc adm policy add-cluster-role-to-user cluster-admin student
		- Keystone 	= enables shared authentication with OpenStack Keystone v3 server
		- LDAP		= validates user & password against LDAPv3 server using simple bind authentication
		- Github 	= validate usernames and passwords against GitHub or the GitHub Enterprise OAuth authentication server.
		- OpenID Connect= OpenID Connect identity provider by using an Authorization Code Flow.
	6. Auth Cluster Administrator (in kubeconfig)
		- embeds an X.509 client certificate that never expires
			command
			INFO Run 'export KUBECONFIG=root/auth/kubeconfig' to manage the cluster with 'oc'.
			export KUBECONFIG=/home/user/auth/kubeconfig
			oc get nodes
			or
			oc --kubeconfig /home/user/auth/kubeconfig get nodes
		- authenticate as the kubeadmin virtual user. Successful authentication grants an OAuth access token.
			- kubeadmin in kube-system namespaces (secret)
			command
			oc get secret kubeadmin -n kube-system
			oc delete secret kubeadmin -n kube-system
	7. RBAC (Role-based access control)
		RBAC Object	Description
		---------------------------
		Rule		Allowed actions for objects or groups of objects.
		Role		Sets of rules. Users and groups can be associated with multiple roles.
		Binding		Assignment of users or groups to a role.
		Role Level	Description
		---------------------------
		Cluster role	Users or groups with this role level can manage the OpenShift cluster.
		Local role	Users or groups with this role level can manage only elements at a project level.
		1. command
			oc adm policy add-cluster-role-to-user <cluster-role> <username>
			oc policy add-role-to-user <role-name> <username> -n <project>
			oc adm policy remove-cluster-role-from-user <cluster-role> <username>
			 oc adm policy who-can delete user
		2. role
			Default roles		Description
			admin			Users with this role can manage all project resources, including granting access to other users to access the project.
			basic-user		Users with this role have read access to the project.
			cluster-admin		Users with this role have superuser access to the cluster resources. These users can perform any action on the cluster, and have full control of all projects.
			cluster-status		Users with this role can get cluster status information.
			edit			Users with this role can create, change, and delete common application resources on the project, such as services and deployments. These users cannot act on management resources such as limit ranges and quotas, and cannot manage access permissions to the project.
			self-provisioner	Users with this role can create projects. It is a cluster role, not a project role.
			view			Users with this role can view project resources, but cannot modify project resources.
		3. User Type
			- regular user
			- system user 		= System user names start with a system: prefix, such as system:admin, system:openshift-registry, and system:node:node1.example.com.
			- service accounts	= System account user names start with a system:serviceaccount:namespace: prefix, such as system:serviceaccount:default:deployer and system:serviceaccount:accounting:builder.
		4. Group Management
			oc adm groups new lead-developers
			oc adm groups add-user lead-developers user1
	8. LDAP Identitiy Provider
		- Configure :
			Openshift trust IdM cert
			oc create configmap ca-config-map --from-file=ca.crt=<(curl http://idm.ocp-${GUID}.example.com/ipa/config/ca.crt) -n openshift-config
			Secret
			oc create secret generic ldap-secret --from-literal=bindPassword=${LDAP_ADMIN_PASSWORD} -n openshift-config
			Oauth Config
			oc apply -f tmp/ldap-cr.yaml
			oc login -u admin -p $(LDAP_ADMIN_PASSWORD)
			---
			apiVersion: config.openshift.io/v1
			kind: OAuth
			metadata:
			  name: cluster
			spec:
			  identityProviders:
			  - name: ldapidp
			    mappingMethod: claim
			    type: LDAP
			    ldap:
			      attributes:
			        id:
			        - dn
			        email:
			        - mail
			        name:
			        - cn
			        preferredUsername:
			        - uid
			      bindDN: "uid=admin,cn=users,cn=accounts,dc=ocp4,dc=example,dc=com"
			      bindPassword:
 			       name: ldap-secret
			      ca:
			        name: ca-config-map
			      insecure: false
			      url: "ldaps://idm.ocp4.example.com/cn=users,cn=accounts,dc=ocp4,dc=example,dc=com?uid"
		- Syncronizing LDAP Group (Can be Cronjob) 
			LDAPSyncConfig = resource contains the settings that the OpenShift cluster needs for group synchronization
			oc adm groups sync-config tmp/ldap-sync.yml
			oc get group admins -o yaml
			oc adm policy add-cluster-role-to-group cluster-admin admins
			---
			kind: LDAPSyncConfig
			apiVersion: v1
			url: ldaps://idm.ocp4.example.com
			bindDN: uid=ldap_user_for_sync,cn=users,cn=accounts,dc=example,dc=com
			bindPassword: ldap_user_for_sync_password
			insecure: false
			ca: /path/to/ca.crt
			rfc2307:
			    groupsQuery:
			        baseDN: "cn=groups,cn=accounts,dc=example,dc=com"
			        scope: sub
			        derefAliases: never
 			       pageSize: 0
 			       filter: (objectClass=posixgroup)
			    groupUIDAttribute: dn
			    groupNameAttributes: [ cn ]
			    groupMembershipAttributes: [ member ]
			    usersQuery:
 			       baseDN: "cn=accounts,dc=example,dc=com"
			        scope: sub
			        derefAliases: never
			        pageSize: 0
			    userUIDAttribute: dn
			    userNameAttributes: [ cn ]
			    tolerateMemberNotFoundErrors: false
			    tolerateMemberOutOfScopeErrors: true

	9. Securing Kubernetes APIs (Authorization)
	- Role-based access control (RBAC) authorization is preconfigured in OpenShift. 
	  An application requires explicit RBAC authorization to access restricted Kubernetes APIs.
	- A service account is a Kubernetes object within a project. 
	- The service account represents the identity of an application that runs in a pod.
		- Create an application service account.
		- Grant the service account access to the Kubernetes API.
		- Assign the service account to the application pods.
	- API Use Case
		- Monitoring APP 	= need access to watch cluster resources to verify cluster health
		- Controllers		= GitOps tools, such as Argo, have controllers that watch cluster resources that are stored in a repository, and update the cluster to react to changes in that repository.
		- Operator		= Operators automate creating, configuring, and managing instances of Kubernetes-native applications.
	- ClusterRole
		- type : basic-user, cluster-reader, and view
		oc adm policy add-role-to-user cluster-role -z service-account
		oc adm policy add-cluster-role-to-user cluster-role service-account
			specify service account name in spec.serviceAccountName
		---
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRole
		metadata:
		  name: secret-reader
		rules:
		- apiGroups: [""] 1
		  resources: ["secrets"] 2
		  verbs: ["get", "watch", "list"] 3
	- Accessing API Resources in the Same Namespace
	  	- you need a role or a cluster role and a service account in that namespace.
	- Accesing API Resources in a Different Namespace
		- create the role binding in the project with the resource.
			- create an app-sa service account in the project-1 project.
			- Assign the app-sa service account to your application pod.
			- Create a role binding on the project-2 project that references the app-sa service account and the secret-reader role or cluster role.
				system:serviceaccount:project:service-account
	10. REST API
	   - Method
	     	- Request a token from the OAuth server path /oauth/authorize?client_id=openshift-challenging-client&response_type=token. 
		  The server responds with a 302 Redirect to a location. Find the access_token parameter in the location query string.
		- Log in using the oc login command and inspect the kubeconfig yaml. This is normally located at ~/.kube/config. 
		  Find the token listed under your user entry.
		- Log in using the oc login command, and then run the oc proxy command to expose an API server proxy on your local machine. 
		  Any requests to the proxy server will identify as the user logged in with oc.
		- Log in using the oc login command, and then run the command oc whoami -t.
	  - API Verb
		API Verb	HTTP Request Command	Description
		Create		POST			Create a new resource.
		Get/List	GET			Retrieve a single resource or a list of resources.
		Update		PUT			Update a resource by providing a full specification.
		Patch		PATCH			Update a resource by providing a partial change described in a patch operation.
		Delete	DELETE				Delete a resource or a list of resources.
	  - Access
		curl -k --header "Authorization: Bearer qfyV5ULvv...i712kJT" -X GET https://api.example.com/api
	  - Find REST API
		curl -k --header "Authorization: Bearer qfyV5ULvv...i712kJT" -X GET https://api.example.com/oapi
	  - Filtering output
		curl -sk --header "Authorization: Bearer qfyV5ULvv...i712kJT" \
  -X GET https://api.example.com/api/v1/services | jq '.items[].metadata.name'
	MAINTENANCE TASK
	1. Kubernetes Batch API Resources
		- Job 		= executed once
			oc create job --dry-run=client -o yaml test --image=registry.access.redhat.com/ubi8/ubi:8.6 -- curl https://example.com
		- Cron Job	= scheduled task
			oc create cronjob --dry-run=client -o yaml test --image=registry.access.redhat.com/ubi8/ubi:8.6 --schedule='0 0 * * *' -- curl https://example.com
	NETWORK SECURITY (HTTPS)
	1. Secure Route (TLS Cerificate)
		- Edge Termination 	= terminate in router not to pods (router serving cert)
			client ---- encrypted ---- edge route (crt & key) ----- not encrypted ---- application
			oc create route edge --service api-frontend --hostname api.apps.acme.com --key api.key --cert api.crt  1 2
		- Passthrough		= encrypted traffic is sent straight to pod without TLS (app serving cert)
			client ------ application ----- container cert mounts (/usr/local/etc/ssl/certs)  
		- Re-encryption		= router terminates TLS with cert, and re-encrypted its connection to end point (two cert)
			client ---- encrypted ---- edge route (crt & key) ----- encrypted (Public Key Infrastructure[PKI]) ---- application
		- External  Use (CA approve not wildcard[*.apps.ocp4.example.com])
			Ingress 	= route traffic into the OpenShift environment using recognized CA certificated
			- Create a new configuration map in the openshift-config namespace.
			- Prefix the file path with ca-bundle.crt= to name the data key in the configuration map as ca-bundle.crt.
			-  OpenShift components communicate with each other using external-facing URLs. (NOT RHOCS trust bundle)
				oc create configmap <CONFIGMAP-NAME> --from-file ca-bundle.crt=<PATH-TO-CERTIFICATE> -n openshift-config
				oc patch proxy/cluster --type=merge --patch='{"spec":{"trustedCA":{"name":"<CONFIGMAP-NAME>"}}}'
				oc create secret tls <SECRET-NAME> --cert <PATH-TO-CERTIFICATE> --key <PATH-TO-KEY> -n openshift-ingress
				oc patch ingresscontroller.operator/default -n openshift-ingress-operator --type=merge --patch='{"spec": {"defaultCertificate": {"name": "<SECRET-NAME>"}}}'
				watch oc get pods -n openshift-ingress
		- Replacing Master API Cert
			- The OpenShift master API uses a different certificate than the certificate used by the ingress controller. 
			- Changing the master API certificate allows users to log in securely using the oc command.
		- To change the master API certificate, you need:
			- The master API certificate and key in PEM format.
			- The certificate is issued to the URL used to access the master API, such as api.ocp4.example.com.
			- The subjectAltName extension for the certificate contains the URL used to access the master API, such as DNS:api.ocp4.example.com.
				oc create secret tls <SECRET-NAME> --cert <PATH-TO-CERTIFICATE> --key <PATH-TO-KEY> -n openshift-config
				oc patch apiserver cluster --type=merge -p '{"spec": {"servingCerts": {"namedCertificates":''[{"names": ["<API-SERVER-URL>"],''"servingCertificate": {"name": "<SECRET-NAME>"}}]}}}'
				oc get clusteroperator/kube-apiserver
		- Enterprise CA (config.openshift.io/inject-trusted-cabundle=true)
			- Copy your enterprise CA certificate to the /etc/pki/ca-trust/source/anchors directory. 
			  Change the name of the certificate if it conflicts with an existing file name in that directory.
			- Run the update-ca-trust extract command.
				oc whoami --show-console
				oc create route edge <ROUTE-NAME> --service <SERVICE>
			- Check Enterprise CA trusted in Openshift
				oc get proxy/cluster -o jsonpath='{.spec.trustedCA.name}{"\n"}'
				oc extract configmap <CONFIGMAP-NAME> -n openshift-config --confirm
				cat ca-bundle.crt
				oc set data configmap <CONFIGMAP-NAME> --from-file ca-bundle.crt=<PATH-TO-NEW-CERTIFICATE> -n openshift-config
				oc label configmap <CONFIGMAP-NAME> config.openshift.io/inject-trusted-cabundle=true
				oc set volume dc/<DC-NAME> -t configmap --name trusted-ca --add --read-only=true --mount-path /etc/pki/ca-trust/extracted/pem --configmap-name <CONFIGMAP-NAME>
				oc rsh <hello-3-65qs7>
				curl https://hello.apps.ocp4.example.com
	2. Network Policies (ingress or egress rule)
		- developer previledge enough
		- using label (spec.ingress.namespaceSelector.matchLabels.network) (spec.ingress.podSelector.matchLabels.role) (port)instead IP addresses
		- connection between two pod different namespaces
		- command 
		  add label to namespaces
		  oc label namespace network-1 network=network-1
			kind: NetworkPolicy
			apiVersion: networking.k8s.io/v1
			metadata:
			  name: network-1-policy
			  namespace: network-1
			spec:
			  podSelector:  1
			    matchLabels:
			      deployment: product-catalog
			  ingress:  2
			  - from:  3
			    - namespaceSelector:
			        matchLabels:
			          network: network-2
				      podSelector:
				        matchLabels:
				          role: qa
			    ports:  4
			    - port: 8080
			      protocol: TCP
		allow access from Openshift Cluster SVC
			---
			apiVersion: networking.k8s.io/v1
			kind: NetworkPolicy
			metadata:
			  name: allow-from-openshift-ingress
			spec:
			  podSelector: {}
			  ingress:
			  - from:
			    - namespaceSelector:
			        matchLabels:
			          network.openshift.io/policy-group: ingress
			---
			apiVersion: networking.k8s.io/v1
			kind: NetworkPolicy
			metadata:
			  name: allow-from-openshift-monitoring
			spec:
			  podSelector: {}
			  ingress:
			  - from:
			    - namespaceSelector:
			        matchLabels:
			          network.openshift.io/policy-group: monitoring
	3. Interlal Traffic with TLS
		- Zero-trust environments require that a trusted certificate authority (CA) signs the certificates that are used to encrypt traffic. 
		- By referencing the CA certificate, an application can cryptographically verify the authenticity of another application with a signed certificate.
		- OpenShift provides the service-ca controller to generate and sign service certificates for internal traffic
		- To generate a certificate and key pair, apply the service.beta.openshift.io/serving-cert-secret-name=your-secret annotation to a service. 
			oc annotate service hello service.beta.openshift.io/serving-cert-secret-name=hello-secret
		  mount the secret in the application deployment. 
			spec:
			  template:
			    spec:
			      containers:
			        - name: hello
			          volumeMounts:
			            - name: hello-volume 1
 			             mountPath: /etc/pki/nginx/ 2
			      volumes:
 			       - name: hello-volume 3
			          secret:
			            defaultMode: 420 4
			            secretName: hello-secret 5
			            items:
			              - key: tls.crt 6
			                path: server.crt 7
			              - key: tls.key 8
 			                path: private/server.key 
		- Client Service Application Configuration
		  - Config Map
		    Apply the service.beta.openshift.io/inject-cabundle=true annotation to a configuration map to inject the CA bundle into the data: { service-ca.crt }
			oc annotate configmap ca-bundle service.beta.openshift.io/inject-cabundle=true
		  - API service
		    Applying the annotation to an API service injects the CA bundle into the spec.caBundle field.
		  - CRD
		    Applying the annotation to a CRD injects the CA bundle into the spec.conversion.webhook.clientConfig.caBundle field.
		  - Mutating or validating webhook
		    Applying the annotation to a mutating webhook or validating webhook injects the CA bundle into the clientConfig.caBundle field.
		- Key Rotation
		  - valid 26 months, rotate after 13 month, 13 month after is grace period (pod need restarted to inject new CA bundle)
		  - manual rotate
			oc delete secret certificate-secret (restart pod if needed)
			oc delete secret/signing-key -n openshift-service-ca
	4. Alternatives to Service Cert
	   - Service Mesh
	   - certmanager operator
	5. Troobleshoot Certificate
		Resources :
			- secrets
			- configMaps
			- custom resources (apiserver)
		Check	:
			watch oc get clusteroperator/kube-apiserver
			oc get events --sort-by='.lastTimestamp' -n openshift-kube-apiserver
			oc get pods -n openshift-ingress
		Renew Certificate :
			- API Server
			oc get apiserver/cluster -o yaml <<<< find .spec.servingCerts.namedCertificates
			oc extract secret/<SECRET-NAME> -n openshift-config --confirm
			openssl x509 -in tls.crt -noout -dates
			oc set data secret <SECRET-NAME> --from-file tls.crt=<PATH-TO-NEW-CERTIFICATE> --from-file tls.key=<PATH-TO-KEY> -n openshift-config
			- Ingress Controller
			oc get ingresscontroller/default -n openshift-ingress-operator -o jsonpath='{.spec.defaultCertificate.name}{"\n"}'
			oc extract secret/<SECRET-NAME> -n openshift-ingress --confirm
			openssl x509 -in tls.crt -noout -dates
			oc set data secret <SECRET-NAME> --from-file tls.crt=<PATH-TO-NEW-CERTIFICATE> --from-file tls.key=<PATH-TO-KEY> -n openshift-config
			curl -v -k https://oauth-openshift.apps.ocp4.example.com 2>&1 | grep -w date
			- cluster Proxy
			oc get proxy/cluster -o jsonpath='{.spec.trustedCA.name}{"\n"}'
			oc set data configmap <CONFIGMAP-NAME> --from-file ca-bundle.crt=<PATH-TO-NEW-CERTIFICATE> -n openshift-config
	EXPOSING NON-HTTP SERVICE (LOAD BALANCER)
	- accesing app that need ip and port
	  a service that uses the 1.2.3.4 IP address runs an SSH server that listens on port 22.
	- A service resource contains the following information:
	  	- A selector that describes the pods that run the service
	  	- A list of the ports that provide the service on the pods
	1. Internal communication
	  Services of the ClusterIP type provide service access within the cluster.
	2. Exposing services externally
		Services of the NodePort and LoadBalancer types, as well as the use of the external IP feature of ClusterIP services, 
		expose services that are running in the cluster to outside the cluster.
	3. Load Balancer SVC
		- Metal LB operator
		  load balancer component that provides a load balancing service for clusters that do not run on a cloud provider
		  MetalLB operates in two modes: layer 2 and Border Gateway Protocol (BGP), with different properties and requirements.
		- Load Balancer SVC
			oc expose --type LoadBalancer
			oc get svc
			-----
			apiVersion: v1
			kind: Service
			metadata:
			  name: example-lb
			  namespace: example
			spec:
			  ports:
			  - port: 1234 1
			    protocol: TCP
			    targetPort: 1234
			  selector:
			    name: example 2
			  type: LoadBalancer 3

	MULTUS SECONDARY NETWORK
	- expose app externaly using secondary network (for security concern)
	- The Multus CNI (container network interface) plug-in helps to attach pods to custom networks.
	1. Configuring Secondary Network
		- use operators, such as the Kubernetes NMState operator or the SR-IOV (Single Root I/O Virtualization) network operator
		- The SR-IOV network operator configures SR-IOV network devices for improved bandwidth and latency on certain platforms and devices.
	2. Attaching Secondary Network
		- create a NetworkAttachmentDefinition resource
		- pod annotation
			- add the k8s.v1.cni.cncf.io/networks annotation to the pod's template
				apiVersion: apps/v1
				kind: Deployment
				metadata:
				  name: example
				  namespace: example
				spec:
 				 selector:
				    matchLabels:
				      app: example
				      name: example
				 template:
				    metadata:
				      annotations:
				        k8s.v1.cni.cncf.io/networks: example
				      labels:
				        app: example
				        name: example
 			- oc get pod example -o jsonpath='{.metadata.annotations.k8s\.v1\.cni\.cncf\.io/networks-status}'
	3. Attachment Custom Resources
	  Host device
	  Attaches a network interface to a single pod.
		apiVersion: k8s.cni.cncf.io/v1
		kind: NetworkAttachmentDefinition
		metadata:
		  name: example 1
		spec:
		  config: |-
		    {
		      "cniVersion": "0.3.1",
		      "name": "example", 2
		      "type": "host-device", 3
		      "device": "ens4",
		      "ipam": { 4
		        "type": "dhcp"
			--or--
			"ipam": {
			  "type": "static",
			  "addresses": [
			    {"address": "192.168.X.X/24"}
			  ]
			}
		      }
		    }
	  Bridge
	  Uses an existing bridge interface on the node, or configures a new bridge interface. 
	  The pods that are attached to this network can communicate with each other through the bridge, and to any other networks that are attached to the bridge.
	  IPVLAN
	  Creates an IPVLAN-based network that is attached to a network interface.
	  MACVLAN
	  Creates an MACVLAN-based network that is attached to a network interface.

	4. Network Operator Settings
	  You can also create the same network attachment by editing the cluster network operator configuration:
		apiVersion: config.openshift.io/v1
		kind: Network
		metadata:
		  name: cluster
		spec:
		...output omitted...
		  additionalNetworks:
		  - name: example 1
		    namespace: example 2
		    rawCNIConfig: |- 3
		      {
		        "cniVersion": "0.3.1",
		        "name": "example", 4
		        "type": "host-device", 5
		        "device": "ens4",
		        "ipam": { 6
		          "type": "dhcp"
		        }
 		     }
  		  type: Raw

	NODE CERTIFICATES
	1. Node Scaling
		- Machine API automatically performs scaling operations 
		- base steps remain the same (RHCOS):
			- Update the compute node Ignition file with an updated TLS certificate.
			- Install Red Hat Enterprise Linux CoreOS (RHCOS) from an ISO image or using a Preboot eXecution Environment (PXE) boot.
			- Add the new instance to the ingress load balancer.
			- Approve Certificate Signing Requests (CSRs) to allow the compute node to join the cluster.
		- Update TLS Certificate
			- This certificate expires 24 hours after installation (ignition).
			- Fetch the latest TLS certificate from the Machine Config Server listening on the control plane port 22623.
			- Run the openssl s_client -connect api-int.ocp4.example.com:22623 -showcerts command to retrieve the certificate.
			- Extract the text wrapped between the BEGIN CERTIFICATE and END CERTIFICATE markers.
			- Replace the base64-encoded certificate in the worker.ign as follows.
			- Use the base64 command to encode the certificate text.
		- Installing with PXE boot
			- PXE relies on a set of very basic technologies:
			- Dynamic Host Configuration Protocol (DHCP) for locating instances.
			- Trivial File Transfer Protocol (TFTP) for serving the PXE files.
			- HTTP for the ISO images and configuration files.
			- The RHCOS PXE configuration specifies the location of a kernel file, OS images, and an Ignition configuration file.
		- Ingress Load Balancing
			- add the instance to external load balancer back-end pools. 
			- HAProxy configuration file /etc/haproxy/haproxy.cfg. 
				frontend ingress_secure
				    bind *:443
				    mode tcp
				    default_backend ingress_secure_backend
				backend ingress_secure_backend
				    balance roundrobin
				    mode tcp
				    server worker01 192.168.50.13:443 check
				    server worker02 192.168.50.14:443 check
		- Approving Cert Signing Req
			- When a new node attempts to join the cluster, OpenShift creates two Certificate Signing Requests (CSRs).
			- First, the Machine Config Operator client requests a CSR: system:serviceaccount:openshift-machine-config-operator:node-bootstrapper
			- Then, after you approve the node bootstrapper CSR, OpenShift creates a CSR with the new node as the requester, such as system:node:worker04
				oc get csr -A
				oc adm certificate approve <csr-name>
				--approve all
				oc get csr -A -o name | xargs oc adm certificate approve
	MACHINE CONFIG & MACHINE CONFIG POOL
	- Machine Config Operator (MCO) manages instance configuration changes and operating system upgrades.
	 1. MachineConfig (MC)
		- Machine Configs declare instance customizations using the Ignition config format. 
		  Machine Configs are labeled with a role such as worker or infra.
		- Machine Configs use the Ignition configuration format.
		- Machine Config Daemon supports a limited set of configuration changes :
			- Configuring core user SSH authorized keys.
			- Declaring Systemd units.
			- Writing custom files. ( Base64 encoded in ignition files)
		- Machine Configs are specified with a machineconfiguration.openshift.io/role label.
			oc get machineconfig --selector=machineconfiguration.openshift.io/role=worker
			---
			apiVersion: machineconfiguration.openshift.io/v1
			kind: MachineConfig
			metadata:
			  labels:
			    machineconfiguration.openshift.io/role: worker 1
			  name: 60-journald 2
			spec:
			  config:
			    ignition:
			      version: 2.2.0
			    storage:
			      files:
			      - contents:
			          source: data:text/plain;charset=utf-8;base64,VGVzdGl...E8gKItAo= 3
			        filesystem: root
 			       mode: 0644
			        path: /etc/systemd/journald.conf
	2. MachineConfigPool (MCP)
		Machine Config Pools use labels to match one or more Machine Configs to one or more nodes. 
		This creates a pool of nodes with the same configuration. 
		The Machine Config Operator uses the Machine Config Pool to track status as it applies Machine Configs to the nodes.
			apiVersion: machineconfiguration.openshift.io/v1
			kind: MachineConfigPool
			metadata:
			    name: ml
			spec:
			    machineConfigSelector:
			        matchExpressions:
 			           - key: machineconfiguration.openshift.io/role
				     operator: In
  			             values: [worker, ml] 1
 			   nodeSelector:
			        matchLabels:
  			      node-role.kubernetes.io/ml: "" 2
	3. Labeling Node
		- Add a custom role to a node by labeling the node with a node-role.kubernetes.io/ROLE
			oc label node/worker03 node-role.kubernetes.io/infra=
			oc get nodes
		- Pod schedulling in speciffic node
			(.spec.template.spec.nodeSelectpr.node-role.kubernetes.io/role: "")

	PATCHING KUBERNETES RESOURCES
		- oc patch deployment hello -p '{"spec":{"template":{"spec":{"resources":{"requests":{"cpu": "100m"}}}}}}'
		- oc patch deployment hello --patch-file ~/volume-mount.yaml

	PROJECT and CLUSTER QUOTAS
	-  By using Kubernetes role-based access control (RBAC), 
	   cluster administrators can allow users to create workloads on their own.
	1. Resources Limits
	   Kubernetes can limit the resources that a workload consumes. 
	   Workloads can specify an upper bound of the resources that they expect to use under normal operation.
	   resource limits prevent the workload from consuming an excessive amount of resources and impacting other workloads.
	2. Resources Requests
	   Workloads can declare their minimum required resources. 
	   Kubernetes tracks requested resources by workloads, and prevents deployments of new workloads if the cluster has insufficient resources.
		apiVersion: v1
		kind: ResourceQuota
		metadata:
		  name: memory
		  namespace: example
		spec:
		  hard: 1
		    limits.memory: 4Gi
		    requests.memory: 2Gi
		  scopes: {} 2
		  scopeSelector: {} 3
	3. Object Count Quotas
		- create a quota that prevents the creation of more than 10 deployments in a namespace.
			oc api-resources --api-group=""  --namespaced=true
	4. Applying Project Quotas
		oc create resourcequota example --hard=count/pods=1
		oc get quota example -o yaml
		oc get quota
		- Across Multiple Projects
		oc create clusterresourcequota example --project-label-selector=group=dev --hard=requests.cpu=10
			---
			apiVersion: quota.openshift.io/v1
			kind: ClusterResourceQuota
			metadata:
			  name: example
			spec:
			  quota: 1
			    hard:
			      limits.cpu: 4
			  selector: 2
			    annotations: {}
			    labels:
			      matchLabels:
 			       kubernetes.io/metadata.name: example
			- AppliedClusterResourceQuota
			  oc describe AppliedClusterResourceQuota -n example-2
	5. Troubleshooting Resource Quotas
		oc create resourcequota example --hard=count/deployment=1
		oc get resourcequota
		oc get event --sort-by .metadata.creationTimestamp

	MANAGING NAMESPACE RESOURCES
	1. Limit Ranges
		Default limit
		Use the default key to specify default limits for workloads.
		Default request
		Use the defaultRequest key to specify default requests for workloads.
		Maximum
		Use the max key to specify the maximum value of both requests and limits.
		Minimum
		Use the min key to specify the minimum value of both requests and limits.
		Limit-to-request ratio
		The maxLimitRequestRatio key controls the relationship between limits and requests. If you set a ratio of two, then the resource limit cannot be more than twice the request.
			 oc set resources deployment example --limits=cpu=new-cpu-limit
			---
			apiVersion: v1
			kind: ResourceQuota
			metadata:
			  name: example
			  namespace: example
			spec:
			  hard:
			    limits.cpu: "8"
			    limits.memory: 8Gi
			    requests.cpu: "4"
			    requests.memory: 4Gi

	PROJECT CREATION (Self-Provisioner Role)
	1. Resources = ProjectRequest
	2. Project Template
		- Roles and Role Bindings
		  grant specific permissions in new projects. 
		- Resource quotas and limit ranges
		- Network policies
		  Add network policies to the template to enforce organizational network isolation requirements.
	3. command
		oc adm create-bootstrap-project-template -o yaml > file (initial template)
			To define a project template and to reduce the risk of errors, you can perform the following steps:
				Create a namespace.
				Create your chosen resources and test until you get the intended behavior.
				List the resources in YAML format.
				Edit the resource listing to ensure that the definitions create the correct resources. 
					For example, remove elements that do not apply to resource creation, such as the creationTimestamp or status keys.
				Replace the namespace name with the ${PROJECT_NAME} value.
				Add the list of resources to the project template that the oc adm create-bootstrap-project-template command generates.	
		oc create -f template -n openshift-config
			--- Update the projects.config.openshift.io/cluster
			apiVersion: config.openshift.io/v1
			kind: Project
			metadata:
			...output omitted...
			  name: cluster
			...output omitted...
			spec:
			  projectRequestTemplate:
			    name: project-request
		oc describe clusterrolebinding.rbac self-provisioners
		---- To make changes, disable automatic updates with the annotation, and edit the subjects in the binding.
		oc annotate clusterrolebinding/self-provisioners --overwrite rbac.authorization.kubernetes.io/autoupdate=false
		oc patch clusterrolebinding.rbac self-provisioners -p '{"subjects": null}'

	DETECT DEPRECATED KUBERNETES API USAGE
	API version	Category	Description
	v1alpha1	Alpha	Experimental features
	v1beta1		Beta	Pre-release features
	v1		Stable	Stable features, generally available
	1. command
		---check
		oc api-resources | egrep '^NAME|cronjobs'
		egrep 'kind|apiVersion' cronjob-beta.yaml
		oc get apirequestcounts | awk '{if(NF==4){print $0}}'
		can't deploy beta but there is stable version (deprecated)
	2. Latest Kubernetes API
		Resource		Removed API Group	Current API Group
		CronJob			batch/​v1beta1		batch/​v1
		Event			events.k8s.io/​v1beta1	events.k8s.io/​v1
		PodSecurityPolicy	policy/​v1beta1		The PodSecurityPolicy admission controller will be removed
		RuntimeClass		node.​k8s.​io/​v1beta1	node.​k8s.​io/​v1
	3. Deprecated and Removed Features in Openshift
		OpenShift 4.10		OpenShift 4.11		OpenShift 4.12	Feature
		General Availability	General Availability	Deprecated	CoreDNS wildcard queries for the cluster.local domain name
		Deprecated		Deprecated		Deprecated	SQLite database format for operator catalogs
		General Availability	Removed			Removed		Removal of Jenkins images from install payload
		General Availability	Removed			Removed		Automatic generation of service account token secrets
		Deprecated		Removed			Removed		Access to Prometheus and Grafana UIs in the monitoring stack

	CLUSTER MONITORING and METRICS
	1. Components	:
		- Cluster Monitoring Operator (CMO)
		The cluster monitoring operator (CMO) is the central component of the monitoring stack. 
		It controls the deployed monitoring components and ensures that they are always in sync with the latest version of the CMO.
			oc get prometheuses -A
			oc get servicemonitors -A
			oc get podmonitors -A
			oc get alertmanagers -A
			oc get prometheusrules -A
			oc adm policy add-cluster-role-to-user cluster-monitoring-view <USER>
		- Prometheus Operator
		The Prometheus operator deploys and configures both Prometheus and Alertmanager. 
		The operator also manages the generation and configuration of configuration targets (service monitors and pod monitors).
		- Prometheus
		Monitoring server
		- Prometheus Adapter
		The Prometheus adapter exposes cluster resources that are used for Horizontal Pod Autoscaling (HPA).
		- Alertmanager
		Alertmanager handles alerts sent by the Prometheus server.
			State		Description
			Firing		The alert rule evaluates to true, and has evaluated to true for longer than the defined alert duration.
			Pending		The alert rule evaluates to true, but has not evaluated to true for longer than the defined alert duration.
			Silenced	The alert is Firing, but is actively being silenced. Administrators can silence an alert to temporarily deactivate it.
			Not Firing	Any alert that is not Firing, Pending, or Silenced is labeled as Not Firing.
		command
			ALERTMANAGER="$(oc get route/alertmanager-main -n openshift-monitoring -o jsonpath='{.spec.host}')"
			curl -s -k -H "Authorization: Bearer $(oc sa get-token prometheus-k8s -n openshift-monitoring)" https://${ALERTMANAGER}/api/v1/alerts | jq .
			oc set data secret/alertmanager-main -n openshift-monitoring --from-file=/tmp/alertmanager.yaml
			oc logs -f -c alertmanager alertmanager-main-0 -n openshift-monitoring
		---alertmanager-main secret
			"global":
			  "resolve_timeout": "5m"
			  "smtp_smarthost": "utility.lab.example.com:25" 1
			  "smtp_from": "alerts@ocp4.example.com" 2
			  "smtp_auth_username": "smtp_training" 3
			  "smtp_auth_password": "Red_H4T@!" 4
			  "smtp_require_tls": false 5
			"receivers":
			- "name": "email-notification" 6
			  "email_configs": 7
			    - "to": "ocp-admins@example.com" 8
			- "name": "default"
			"route":
			  "group_by":
			  - "job"
			  "group_interval": "5m"
			  "group_wait": "30s"
			  "receiver": "default"
			  "repeat_interval": "12h"
			  "routes":
			  - "match":
			      "alertname": "Watchdog"
			    "receiver": "default"
			  - "match":
			      "severity": "critical"
			    "receiver": "email-notification" 9
		- Kube state metrics
		kube-state-metrics is a converter agent that exports Kubernetes objects to metrics that Prometheus can parse.
		- OpenShift state metrics
		openshift-state-metrics is based on the kube-state-metrics and adds monitoring for OpenShift-specific resources (such as image registry metrics).
		- Node exporter
		node-exporter exports low-level metrics for worker nodes.
		- Thanos Querier
		Thanos Querier is a single, multitenant interface that enables aggregating and deduplicating cluster and user workload metrics.
		- Grafana
		Grafana is a platform for analyzing and visualizing metrics. The Grafana dashboards provided with the monitoring stack are read-only.

	CLUSTER LOGGING
	1. Benefits 	:
		aggregation of all the logs from the pods and node of an OpenShift cluster to a centralized location. 
	2. Cluster Logging Components :
		logStore =  is the Elasticsearch cluster
			- Stores the logs into indexes.
			- Provides RBAC access to the logs.
			- Provides data redundancy.
		collection
		Implemented with Fluentd, the collector collects node and application logs, adds pod and namespace metadata, and stores them in the logStore. 
		The collector is a DaemonSet, so there is a Fluentd pod on each node.
		visualization
		The centralized web UI from Kibana displays the logs and provides a way to query and chart the aggregated data.
		event routing
		The Event Router monitors the OpenShift events API and sends the events to STDOUT so the collector can forward them to the logStore. 
		The events from OpenShift are stored in the infra index in Elasticsearch.
	3. Installing Elasticsearch Operator
		Each node in the Elasticsearch cluster is deployed with a PVC named and managed by the Elasticsearch
		A unique Deployment object is created for each Elasticsearch node to ensure that each Elasticsearch node has a storage volume of its own.
		Elasticsearch Operator must be installed in a namespace other than openshift-operators
			---
			apiVersion: v1
			kind: Namespace
			metadata:
			  name: openshift-operators-redhat
			  annotations:
			    openshift.io/node-selector: ""
			  labels:
			    openshift.io/cluster-monitoring: "true"
			---
			apiVersion: operators.coreos.com/v1
			kind: OperatorGroup
			metadata:
			  name: openshift-operators-redhat
			  namespace: openshift-operators-redhat
			spec: {}
			---
			apiVersion: operators.coreos.com/v1alpha1
			kind: Subscription
			metadata:
			  name: "elasticsearch-operator"
			  namespace: "openshift-operators-redhat"
			spec:
			  channel: "4.6"
			  installPlanApproval: "Automatic"
			  source: "redhat-operators"
			  sourceNamespace: "openshift-marketplace"
			  name: "elasticsearch-operator"
			---
			apiVersion: rbac.authorization.k8s.io/v1
			kind: Role
			metadata:
			  name: prometheus-k8s
			  namespace: openshift-operators-redhat
			rules:
			  - apiGroups:
			      - ""
			    resources:
			      - services
			      - endpoints
			      - pods
			    verbs:
			      - get
			      - list
			      - watch
			---
			apiVersion: rbac.authorization.k8s.io/v1
			kind: RoleBinding
			metadata:
			  name: prometheus-k8s
			  namespace: openshift-operators-redhat
			roleRef:
			  apiGroup: rbac.authorization.k8s.io
			  kind: Role
			  name: prometheus-k8s
			subjects:
			  - kind: ServiceAccount
			    name: prometheus-k8s
			    namespace: openshift-operators-redhat
			---
			oc get csv -A
			---
			apiVersion: v1
			kind: Namespace
			metadata:
			  name: openshift-logging
			  annotations:
			    openshift.io/node-selector: ""
			  labels:
			    openshift.io/cluster-logging: "true"
			    openshift.io/cluster-monitoring: "true"
			---
			apiVersion: operators.coreos.com/v1
			kind: OperatorGroup
			metadata:
			  name: cluster-logging
			  namespace: openshift-logging
			spec:
			  targetNamespaces:
			    - openshift-logging
			---
			apiVersion: operators.coreos.com/v1alpha1
			kind: Subscription
			metadata:
			  name: cluster-logging
			  namespace: openshift-logging
			spec:
			  channel: "4.6"
			  name: cluster-logging
			  source: redhat-operators
			  sourceNamespace: openshift-marketplace
			---
			oc get csv -n openshift-logging
		4. Configuration ElasticSearch
			Redundancy mode 	Description
			FullRedundancy 		Elasticsearch copies the shards for each index to every data node in the cluster. Fully replicated shards provide the most redundancy to protect from accidental data loss.
			MultipleRedundancy 	The primary shards for each index are replicated to half of the data nodes. This provides a good balance between performance and redundancy.
			SingleRedundancy 	Each primary shard is copied once to another node. If at least two data nodes remain available, then the logs are recoverable. If the Elasticsearch cluster has 5 or more nodes, SingleRedundancy performs better than MultipleRedundancy.
			ZeroRedundancy 		Shards are not replicated. Data loss could happen if a node fails.
				oc get clusterlogging -n openshift-logging instance -o yaml
				---
				apiVersion: "logging.openshift.io/v1"
				kind: "ClusterLogging"
				metadata:
				  name: "instance"
				  namespace: "openshift-logging"
				spec:
				  managementState: "Managed"
				  logStore:
				    type: "elasticsearch"
				    elasticsearch:
				      nodeCount: 3
				      nodeSelector:
				        node-role.kubernetes.io/infra: ''
				      storage:
				        storageClassName: "local-blk"
				        size: 50G
				      redundancyPolicy: "MultipleRedundancy"
				  visualization:
				    type: "kibana"
				    kibana:
				      nodeSelector:
				          node-role.kubernetes.io/infra: ''
				      replicas: 1
				  collection:
				    logs:
				      type: "fluentd"
				      fluentd: {}
		5. Role
			Master
			Client
			Data
		6. Event Router
			Kubernetes events are saved in the cluster etcd instance.
			The event router monitors the events API and prints events to standard output so Fluentd can collect them and send them to Elasticsearch. 
			Elasticsearch stores the events in the infra index.
				To install the Event Router:
					- Download the template from OpenShift Container Platform documentation.
					- Use the oc process command to create a manifest file for the Event Router.
					- Use the oc apply command to apply the manifest resources to the OpenShift cluster.
		7. Index Pattern
			Every log message is converted into a JSON document that Elasticsearch stores in an index.
			Elasticsearch applies role-based access control (RBAC) rules to indexes based on OpenShift project permissions. 
			Indexes:
				infra-
				An index with this prefix stores pod log messages for infrastructure projects. 
				An infrastructure project includes project names that have a openshift- or kube- prefix.
				app-
				An index with this prefix stores pod log messages for all projects except infrastructure projects.
				audit-
				An index with this prefix stores audit log messages. 
				Audit logs allow you to review the OpenShift API activity of a user, administrator, or OpenShift component (such as a service account).
		8. Logging Alert
			If a component of the cluster logging infrastructure fails, then cluster users lose the capability to diagnosis application issues from logs.
			Fluentd is the component collecting the logs.
			Prometheus scrapes metrics from Fluentd, such as availability, queue length, and errors. 
			After collecting the logs, Fluentd forwards them to Elasticsearch for storage.
			Prometheus also scrapes metrics from Elasticsearch, such as health and resource utilization.
			If Elasticsearch consumes too much disk space, consider:
				Modifying log retention.
				Extending Elasticsearch storage.
			Alert
				Alert					Description
				FluentdErrorsHigh 			Fluentd is reporting more than one error per minute.
				FluentdQueueLengthIncreasing 		The log buffer is growing faster than Fluentd can process.
				ElasticsearchNodeDiskWatermarkReached 	Available disk space is low.
				ElasticsearchProcessCPUHigh 		The CPU usage is above 90%.
			oc get prometheusrules -n openshift-logging
			--- (es_fs_path_available_bytes)
			oc get prometheusrules -n openshift-logging fluentd -o yaml
			kind: PrometheusRule
			...output omitted...
			spec:
			  groups:
			  - name: logging_fluentd.alerts
			    rules:
			    - alert: FluentdNodeDown
			...output omitted...
			    - alert: FluentdQueueLengthIncreasing
			      annotations:
			        message: In the last 12h, fluentd {{ $labels.instance }} buffer queue length
			          constantly increased more than 1. Current value is {{ $value }}.
			        summary: Fluentd file buffer usage issue
			      expr: |
			        delta(fluentd_output_status_buffer_queue_length[1m]) > 1
			      for: 12h
			      labels:
			        service: fluentd
			        severity: critical
		9. Elastic search Tools (run on elasticsearch pod)
			 es_util
				Queries the Elasticsearch REST endpoint.
			es_cluster_health
				Returns a JSON summary of the Elasticsearch cluster health.
			oc exec -n openshift-logging -c elasticsearch elasticsearch-cdm-giqewkd9-1-5f8b46cdbb-qrqf7 -- es_cluster_health

	RECOVERING FAILED WORKER NODES
	1. Healthy Status
	about the cluster and worker node health are essential to identifying a degraded environment.
		oc get nodes <NODE>
		oc adm top node <NODE>
		oc describe node <NODE> | grep -i taint
		oc whoami --show-console
	2. Service Functionaility
	Each worker node has two main services, kubelet and cri-o
	More severe service issues might require direct access using CLI commands on the worker node, such as systemctl
	Determine the appropriate method to manage the services on worker nodes:
		The web console monitoring provides insight into unhealthy worker nodes, but might not give specific details for every scenario.
		Use the oc client commands to gather as much information as available for the given symptoms.
		If you cannot correct the issue using other means, then connect directly to the worker node using SSH to mitigate the issue.
	Navigate the web console information alerting administrators to node issues.
	3. Woker Node Performance
	When OpenShift detects an adverse node condition, OpenShift applies a corresponding taint to the node.
	A taint that represents an adverse node condition consists of a key value pair directing cluster interactions with the worker node.
	For example, the node.kubernetes.io/disk-pressure
	Effect value		Description
	NoSchedule		New pods that do not match the taint are not scheduled on the node.
				Pods that are running on the node continue running.
	PreferNoSchedule	New pods that do not match the taint are only scheduled on the node when no other suitable node can be located.
				Pods that are running on the node continue running.
	NoExecute		New pods that do not match the taint are not scheduled on the node.
				Pods that are running on the node are removed and deployed on another suitable node.
	4. Storage Capacity Issues
	removing old logs or container images, instills behaviors that result in effective capacity management. 
	Additionally, simulating disaster recovery scenarios and practicing troubleshooting skills directly correlates to overall better cluster health.
	Large or numerous container images consume space and require proper footprint management to avoid worker node storage depletion. 
	applications that are configured to utilize ephemeral storage allocated directly from the local disk of a worker node deplete available capacity.
	One clue that OpenShift provides when worker node storage issues arise is the application of a disk-pressure:NoSchedule and/or disk-pressure:NoExecute taint.
	Traditional system administration tools, such as the logrotate service, are invaluable for maintaining resource allocations.
	Employing quotas to manage application consumption of resources is also prudent in large-scale environments. 
	Curating container images and using good management practices to prune old, stale, or large images reduces storage constraints caused by the growth of the image footprints.
		oc describe node <NODE_NAME>

	UPDATE OPENSHIFT
	1. Over-the-Air updates (OTA)
		- manages = controller manifests, cluster roles, and any other resources to update a cluster to a particular version.
	2. Before action
		oc get machinehealthcheck -n openshift-machine-api
		oc annotate machinehealthcheck -n openshift-machine-api machine-api-termination-handler cluster.x-k8s.io/paused=""
	3. After action
		oc annotate machinehealthcheck -n openshift-machine-api machine-api-termination-handler cluster.x-k8s.io/paused-
	4. Update Process
		- Machine Config Operator
		  The Machine Config Operator applies the desired machine state to each of the nodes.
		  This component also handles the rolling upgrade of nodes in the cluster, and uses CoreOS Ignition as the configuration format.
		- Operator Lifecycle Manager
		  The OLM orchestrates updates to any operators that are running in the cluster.
	5. Update via web console
		Administration > Cluster Settings > Update now
	6. Update via CLI
		*Be sure to update all operators that are installed through the OLM to the latest version before updating the OpenShift cluster.
		*Retrieve the cluster version and review the current update channel information. If you are running the cluster in production, then ensure that the channel reads stable.
			oc get clusterversion
			oc get clusterversion -o jsonpath='{.items[0].spec.channel}{"\n"}'
			oc patch configmap admin-acks -n openshift-config --type=merge --patch '{"data":{"ack-4.11-kube-1.25-api-removals-in-4.12":"true"}}'
			--check upgrade
			oc adm upgrade
			--apply upgarde
			oc adm upgrade --to-latest=true or oc adm upgrade --to=VERSION
			--check process
			oc get clusterversion
			oc describe clusterversion
	7. Proccess
		The updates use the rpm-ostree technology for managing transactional upgrades. 
		Updates are delivered via container images and are part of the OpenShift update process. 
		When the update deploys, the nodes pull the new image, extract it, write the packages to the disk, and then modify the bootloader to boot into the new version. 
		The machine reboots and implements a rolling update to ensure that the cluster capacity is minimally impacted.


	Continous Integration (CI) and Continous Deployment (CD) 
	- workwell with openshift imperative commands
	1. Continous Integration
		- Build Software
			building a software piece from its source code and any required dependencies, such as programming libraries
		- Run Unit Tests
		- Save Artifact
			distributable piece of software, such as a native executable file, an RPM package, a Java library archive (JAR file), or a container image.
	2. Continous Deployment
		- Find/Create Target Environtment (Integration Testing)
		- Deploy Artifact
		- Smoke Test Artifact
	3. GitOps
		GitOps is a process that stores configurations of a target environment, such as a server or a cluster, in files managed by a version control system. 
		It works under the assumption that system administrators do not change configurations on a live system directly. 
		The best practice is to change the configurations under version control, and optionally have a fellow system administrator review them, before applying to a live system.
	4. Jenkins
		Jenkins is a Java application that can be deployed to Servlet containers.
		- Describing Essential Jenkins Concepts
		  Project (or Job)
			A script that describes a workflow that Jenkins should perform, such as building an application.
		  Pipeline
    			A kind of Job that follows the pipeline concept and syntax and describes a workflow as a sequence of steps that are grouped in stages.
		  Build
			A single execution of a project, including its runtime logs and output artifacts. 
			This term comes from Jenkins' origins as a software build server.
		  Node
			A server or container that runs builds.
		  Worker
			A thread in a master node that either runs a build or orchestrates available agents.
		  Workspace
			A file system folder, dedicated to a project and sometimes also to a node, where builds store data that is either temporary, or reused between multiple builds of the same project.
		  Credential
			A Jenkins construct that provides projects and builds with access credentials to external resources. 
			There are different credentials to store user name and password pairs, SSH keys, and other credential types.
		  Plug-in
			Almost all of Jenkins functionality is extensible by plug-ins written in Java. 
			There are many community plug-ins that support different kinds of nodes, credentials, and programming languages.
			*Note that these plug-ins may depend on software tools, such as the Java Development Kit (JDK) 
			and the Node Package Manager (NPM) on agent nodes.
		- Types of Jenkins Nodes
			Master
				Stores definitions of projects and their builds.
			Agent
				Run builds (or parts of a build) under the control of a master node.
		- Jenkinsfile
    			- The pipeline directive requires one agent and one stages directive.
			- The stages directive requires one or more stage directives.
				The following directives are allowed inside pipelines directive:
				triggers: defines conditions that fire automatic execution of builds from that pipeline.
    				options: defines general configuration settings for the pipeline, overriding most properties from the web UI, for example: timeouts for running builds and retention of build logs.
    				parameters: defines parameters that a user, or an upstream pipeline, can provide for running a build.
				environment: defines environment variables available inside a pipeline or a stage.
				agent: defines which agent nodes should execute, either all stages or a single stage of the pipeline.
				Some directives, for example agent, can occur at different levels inside a pipeline, for example, at either pipeline or stage.
			Scripted pipelines 	= start with node directive and define imperative script using Groovy
			Declarative pipeline	= start with pipeline using domain-scale-language (DSL)
		- Deployment
			Patern	: 
				- each development group inside an organization manages its own Jenkins instance, allow jenkins to projjects
				- ne or more dedicated Jenkins instances with either cluster administrator privileges or custom roles that allow limited access to a few cluster operators. 
			Configuring :
				- Web-UI (Manual)
				- Triggering Pipeline
						- pollSCM takes a crontab expression to check a VCS repository periodically for changes. 
						  When it finds changes (new commits or merges) it starts a new build.
						- cron starts new builds according to a crontab expression. 
						  It starts a new build periodically and unconditionally.
						- upstream starts a pipeline build after another pipeline completes a build. 
						  It is a way of concatenating multiple pipelines into a larger workflow.
							triggers {
							  pollSCM ('H/20 * * * *')
							  upstream (upstreamProjects: 'library/main', threshold: hudson.model.Result.SUCCESS)
							}
				- Web Hook
					Jenkins URLs that embed a random secret for authentication and the name of a VCS repository. 
				- Agent
							agent {
					 		 node {
					   		   label 'nodejs'
						  	 }
							}
				- Option
					disableConcurrentBuilds = prevents multiple builds of the same pipeline from building in parallel. 
								  For example, it avoids concurrent and conflicting updates to the same cluster operator in a GitOps pipeline. 
								  With a CI/CD workflow, it could prevent concurrent updates to the same test database.
					buildDiscarder		= removes old builds, their logs, and their artifacts from the Jenkins instance. 
								  If you do not discard these, then your master node could fill up all its configuration folder volume.
					timeout 		= puts a time limit for the duration of a build. If a build takes longer than the designated time limit, then it is aborted and reported as failed.
							options {
							  buildDiscarder (logRotator (numToKeepStr: '30', artifactNumToKeepStr: '30'))
							  disableConcurrentBuilds ()
							  timeout (time: 1, unit: 'HOURS')
							}
				- Stages
					The when directive allows a stage to be executed or skipped under some conditions, for example:
					branch		= specifies the name of a branch that runs the stage in a multibranch pipeline.
					changeset	= specifies that the stage runs for changes affecting a set of files from VCS. 
							  File changes are specified using Maven file set syntax.
					expression	= allows using a Groovy expression to refer to values of parameters and environment variables.
							when {
							  allOf {
							    branch 'master'
							    changeset '*/.js'
							  }
							}
		- Roles Jenkins
			 admin
				Manage the Jenkins instance.
			edit
				Create and edit projects in the Jenkins instance.
			view
				View status, logs, and artifacts of builds from the Jenkins instance.
		- Pipeline Project
			Jenkins UI 	: manage pipeline projects and builds or to add plug-ins, such as the Configuration as Code (CasC) plug-in.
		- Project types 
			Pipeline
			  Runs a pipeline taking as input a single branch from a version control system repository.
			Multibranch pipeline
			  Automatically creates new projects when new branches are detected in a version control system repository. 
			  All these projects share the same pipeline definition that must be flexible enough to avoid conflicts between builds in different branches.
		- Jenkins API
			curl --user 'jenkins-user:token' https://jenkins-host/resource-path
		- DSL CI/CD
			Characteristic
			- The DSL always assumes a current project reads back all resources it creates or changes. 
			  It forces you to break commands that manipulate resources from different namespaces into multiple steps.
			- That behavior works for CI/CD pipelines because they usually build and deploy a single application that lives inside a single OpenShift project.
			- GitOps pipelines frequently mix namespaced and non-namespaced resources, or resources from multiple namespaces (of multiple operators), that a single oc apply command can manage.
			- The DSL increases verbosity for simple declarative GitOps workflows because it is designed for scripted pipelines and requires adding script directives to declarative pipelines.
			- Even if your pipeline never changes OpenShift credentials (it runs as the Jenkins service account) and never changes the project, the DSL requires withCluster and withProject Groovy blocks, further increasing verbosity.
			Pipeline
				stage ('Apply resources') {
				  steps {
				    script {
				      openshift.withCluster () {
				        openshift.withProject ('openshift-config') {
				          openshift.apply ('-k config')
				        }
				      }
				    }
				  }
				}
				--or--
				stage ('Apply resources') {
				  steps {
				    sh 'oc apply -k config'
				  }
				}
	GITOPS (Infrastructure-as-code)
		- All configurations should come from files that are versioned, tested, and reviewed, such as source code in a Continuous Integration and Continuous Deployment (CI/CD) process.
		- do not produce stand alone artifacts, such as binary executables, that you can store, copy, compare, and replace. 
		- Configurations change the state of a live system.
		- Similar to Ansible Playbooks
		1. Benefits
			 - Easier if configuration files for cluster operators, custom resources, and standard resources, such as configuration maps and cron jobs, are stored as either YAML or JSON files in a version control system. 

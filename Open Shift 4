OPENSHIFT
=========
COMPONENT BASED ON KUBERNETES
	kube-proxy			= maintain network between kubernetes resources
	kube-controller-manager		= Governs the state of the cluster
	kube-scheduler			= Allocate pods
	etcd				= Stores cluster data
	kube-apiserver			= Validates and configures data for the API objects
	kubelet				= Read container manifest (makesure container started and running)
	kubectl				= command to interact with kube-apiserver
	container runtime		= install podman / docker
	container-registry		= Stores and accessed container images
	pod				= smallest logical unit in kubernetes, contain one or more containers
	
SCALLING
	HORIZONTAL 			= deploy more instance
	VERTICAL			= Upgrade spek
	
FLOW
USER >>> Controller node (API server) >>> Worker Node {1..3} (Kubelet+Pods)
Contoller node			Compute Node			Storage/Registy
kube-apiserver			kubelet				PV storage
kube-scheduler			kube-proxy			Container registry
kube-controller-manager		CONTAINER RUNTIME
					Container
					Pods
----------------------------------------------------------------------------------
Physical			Virtual				Cloud

KUBERNETES RESOURCES
	Service				= expose a running application on a set of pods
	ReplicaSets			= maintain the constant pod number
	Deployment			= maintain the life cycle of an application

CLI
INSTALL OPENSHIFT CLIENT IN DESKTOP
	1. Download openshift client from redhat
		download
		- curl -LO "https://dl.k8s.io/release/$(curl -L \
  			-s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
		checksum
		- curl -LO "https://dl.k8s.io/$(curl -L \
			-s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
		check
		- echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
		install
		- sudo install -o root -g root -m 0755 kubectl \
  			/usr/local/bin/kubectl
	2. tar xvzf <file>
	3. echo $PATH
	4. mv <file> to $PATH directory
	
LOGIN
	LOGIN OCP only
	1. oc login -u user1
	LOGIN TO PROJECT
	2. oc login <optional HTTP/HTTPS/No proxy> -u user1
	3. oc cluster-info
	4. oc api-versions
	5. oc get clusteroperator
	
PROJECT
	1. oc new-project <project name>
	2. oc projects
	3. oc status
	
POLICY
	1. oc adm policy add-role-to-user <admin> <nama_user/iduser> -n <prod-mashery>
	   for f in nama/id_user nama/id_user nama/id_user; do oc adm policy \
	   add-role-to-user admin $f -n <prod-mashery>; done;
	2. oc get rolebindings -owide -n <prod-mashery> | grep nama/id_usr
	
APPLICATION
	1. oc new-app <Image link/Container link>
	2. oc create route edge <name route> --service=<name service>
	3. oc get all

OPERATOR (cluster-admin)
	CLUSTER VERSION OPERATOR (CVO)
	1. oc get clusteroperator / oc get co
	2. oc describe clusteroperator <operator name>
	3. oc explain
	OPERATOR LIFECYCLE MANAGER (OLM)
	addon
	
VIEW PODS
	1. oc get pods -o wide -A (all namespaces)
		i. oc get pod -A --sort-by='{.metadata.creationTimestamp}' --show-labels
	2. oc logs <pod name>
	3. oc describe pods <pod name>
	4. oc get service
	5. oc get pods <pods name> -o yaml
	   ex:
	   oc get pods -o yaml | yq r - 'items[0].status.podIP'  <<<< filter [index in the item array]
	   oc get pods \
	   -o custom-columns=PodName:".metadata.name",\
	   ContainerName:"spec.containers[].name",\
	   Phase:"status.phase",\
	   IP:"status.podIP",\
	   Ports:"spec.containers[].ports[].containerPort"
	6. oc explain
	7. oc adm top pods -A --sum
	8. oc adm top pods <pod name> -n <namespaced> --containers
	9. oc logs pod-name -c <container name>
	10. oc debug
	11. oc debug job/<job name> --as-user=100000
RUN PODS
	1. oc run <name pod> --image <imagename> --it(interaktif) --command -- <command executed> -- restart <restart option> -- env <env values>
	ex:
	oc run mysql --image registry.redhat.io/rhel9/mysql-80 --env MYSQL_ROOT_PASSWORD=myP@$$123
	2. oc exec <podname> <option> -- <command>
	ex:
	oc exec my-app -c ruby-container -- date
	oc exec my app -c ruby-container -it -- bash -il
LOGS POD
	oc logs <pod name> -l/--selector='' --tail= -c/--container= -f/--follow
	
DELETE PODS
	1. oc delete pod <name pod> -n <namespace> -l <label> --grace-period=
	2. ns=<NAMESPACE>; for i in $(oc get pod -n $ns | grep -i completed | awk '{print $1}'); do oc delete pod $i -n $ns;done
	
CLUSTER EVENTS and ALERTS
	EVENTS
	1. oc get events -n <namespaces>
	2. oc describe pod <pod name>
	ALERTS (openshift-monitoring namespace)
	1. oc get all -n <namespace> --show-kind
	2. oc logs <pod alert>

CLUSTER NODE STATUS
	1. oc cluster-info
	2. oc get nodes
	   ex:
	   oc get node master01 -o json | jq '.status.conditions'
	3. oc adm node-logs --role <role name> -u <specified unit> --path=<command> --tail <baris>
	   ex:
	   oc adm node-logs --role master -u kubelet --path=cron --tail 1
	4. oc debug node/<node-name> <<<< accest via kubelet
	5. chroot /host
	6. oc adm must-gather --dest-dir </path>
		tar cvaf <file must gather>
	7. oc adm inspect <specified resources> --since <last time>
	NODE CONDITION
	CONDITION			DESCRIPTION
	OutOfDisk			If true, has insufficient free space for adding new pods
	NetworkUnavailable		If true, network not correctly configured
	NotReady			If true, one of components, such as container runtime or network is experiencing issue or not configured
	SchedullingDisabled		If true, Pods cannot be scheduled for placement on the node
	
	
SCALING THE APPLICATION
	1. oc scale --current-replicas=1 --replicas=2 deployment/parksmap
	
	DEPLOY BACK-END SERVICE(Python)
		1. oc new-app python~https://github.com/openshift-roadshow/nationalparks-py.git --name nationalparks -l 'app=national-parks-app,component=nationalparks,role=backend,app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=python' --allow-missing-images=true
	CONNECTING TO A DATABASE
		1. oc new-app quay.io/centos7/mongodb-36-centos7 --name mongodb-nationalparks -e MONGODB_USER=mongodb -e MONGODB_PASSWORD=mongodb -e MONGODB_DATABASE=mongodb -e MONGODB_ADMIN_PASSWORD=mongodb -l 'app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=mongodb'
	SECRET
		1. oc create secret generic nationalparks-mongodb-parameters --from-literal=DATABASE_SERVICE_NAME=mongodb-nationalparks --from-literal=MONGODB_USER=mongodb --from-literal=MONGODB_PASSWORD=mongodb --from-literal=MONGODB_DATABASE=mongodb --from-literal=MONGODB_ADMIN_PASSWORD=mongodb
		2. oc set env --from=secret/nationalparks-mongodb-parameters deploy/nationalparks
		3. oc rollout status deployment nationalparks
		4. oc rollout status deployment mongodb-nationalparks
		
	DEPLOY MYSQL
		1. oc new-app mysql MYSQL_USER=user1 MYSQL_PASSWORD=mypa55 MYSQL_DATABASE=testdb -l db=mysql
	DEPLOY From Docker
		1. oc new-app --docker-image=docker.io/indr01/nama_image:tag --name=nama_app
	DEPLOY From Github
		1. oc new-app https://github.com/openshift/ruby-hello-world --name=ruby-hello
		
ANY UID
	1. oc create sa anyuid-nama_pod
	   ex: oc create sa anyuid-httpd
	oc adm policy add-scc-to-user anyuid -t anyuid-nama_pod
	   ex: oc adm policy add-scc-to-user anyuid -z httpd
Perintah di atas akan memberikan role "AnyUID" kepada pod dengan nama "my-pod". Anda juga dapat menggunakan perintah oc adm policy add-scc-to-group untuk memberikan role "AnyUID" kepada sekelompok pod.

oc adm set sa dc/httpd anyuid-httpd
oc logs nama_pods //melihat logs
contoh : oc logs httpd-2-cbs29

API-RESOURCES
	1. oc api-resources
	
IMPORT-IMAGE
	1. oc import-image my-ruby
	
BUILD 
	1. oc new build <Git directory>
		from build config
		1. oc start-build python

DEPLOY
	Template (YAML manifest in openshift namespace)
		1. oc process -f <templateYAML.yaml> -o yaml 
			1.1. oc process -f <templateYAML.yaml> --parameters
	CREATE DEPLOY
		1. kubectl apply -f <file/filedir/adress>.yaml
			REQ FIELD
			1. apiVersion			= v1/v2/v3
			2. kind				= Deployment
			3. metadata
				1. name
				2. UID
				3. namespace
			4. spec
		2. kubectl get deployments
		3. oc new-app --template <template name>
			3.1. oc new-app --template <templatename> --param <ENV=Par> --image <image>
			3.2. oc new-app <reponame>
		4. oc create deployment <deployname> --image <image>
		5. oc run <pod name> --image=<image> --env <Env=Par> --port 
	ROLLOUT
		3. kubectl rollout status deployment/<deployment-name>
	REPLICASET
		4. kubectl get rs
		ex : [deployment]-[name]-[hash]
		nginx-deployment-979797
	LABEL
		5. kubectl get pods --show-labels
	UPDATE DEPLOYMENT
		6. kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
		7. oc set env deployment/<deploy-name> <env=<par>>
	DESCRIBE DEPLOYMENT
		8. kubectl describe deployments
		9. oc describe template mysql-epemeral -n openshift
	SCALING
		10. kubectl scale deployment/<deplyoment name> --replicas=10 
		if horizontal pos scalling enabled
			kubectl scale deployment/<deployment name> --min=10 --max=15 --cpu-percent=80
			oc scale --replicas 5 deployment/scale
	
TAG
	1. oc tag ruby:latest ruby:2.0

RESOURCE
	Types:
	1. pod (Pods)
	2. svc (Services)
	3. rs (ReplicaSet)
	4. pv (Persitent Volumes)
	5. pvc (Persistent Volume Claims)
	6. cm (ConfigMaps) and Secrets
	7. deploy (Deployment)
	8. bc (BuildConfig)
	9. dc (DeploymentConfig)
	10. routes
	every resource has .kind .apiVersion .status
	ex:
		pod.kind pod.api.Version .status
	Command:
	1. oc api-resources
	ex:
		oc api-resources --namespaced=true --api-group apps --sort-by name
	Condition Resource fields
	Field			Example				Description
	Type			ContainerReady			The Type of the condition
	Status			False				The state of the condition
	Reason			RequirementsNotMet		An optional field to provide extra info
	Message			2/3 container are running	An optional textual desc for condition
	LastTransitionTime	2023-03-07T18:05:28Z
	API Resource field
	Field			Example				Desc
	apiVersion		v1				Identifier of the object schema version
	kind			Pod				Schema Identifier
	metadata.name		<name>				Creates a label with a name key
	metadata.namespace	<namespace>			The namespace located
	metadata.labels		<label>	: app: group:		Key-value pairs identifieying metadata
	
APPLY CONFIG
	1. oc apply -f pod.json
	2. oc create -f pod.json
	3. oc delete pod/<podname>
	4. oc delete pods -l app=parksmap-katacoda
	
AUTOSCALE
	1. oc autoscale deploymentconfig/parksmap-katacoda --min=2 --max=5

ROLLBACK
	1. oc rollback php (--to-version=3)
	
ROLLOUT
	1. oc rollout undo deploymentconfig/php
	
EDIT
	1. oc edit deploymentconfig/<podname>
	2. OC_EDITOR="nano" oc edit deploymentconfig/<podname>
	3. oc edit deploymentconfig/<podname> -o json
	
EXPOSE (Make deployment to create service)
	1. oc expose service/<podname>
	2. oc expose service/<podname> --hostname=www.my-host.com
	
GET
	1. oc get pods -n default
	2. oc get nodes
	3. oc get deploymentconfig/python -o json
	
HELP
	1. oc help 
	or
	oc create --help
	2. oc explain pods
	
LOGOUT
	1. oc logout

CONTAINER IMAGES
	1. ENV
		- define the available env variable 
	2. ARG
		- define build-time variables
	3. USER
		- define the active user in container (to prevent using root)
	4. ENTRYPOINT
		- define the executeable to run when container started
	5. CMD
		- define command when start based on ENTRYPOINT
	6. WORKDIR
		- set of current working dir within container
	7. METADATA
	   EXPOSE
		- define where to store data outside container
	   VOLUME
		- where to store data outside the container
	   LABEL
		- add a key-value pair

TROUBLESHOOTING TOOLS OCP
	1. kubectl describe 	: describe resources
	2. kubectl edit 	: edit config resources
	3. kubectl patch	: update specific attribute or field for a resources
	4. kubectl replace	: deploy a new instance of the resource
	5. kubectl cp		: copy from or to container
	6. kubectl exec		: execute command within a specified container
	7. oc status		: display status container
	8. oc explain		: display documentation
	9. oc rsync		: syncronize files and dir
	10. oc rsh		: start remote shell within a specified container
	11. oc port-forward	: configure port forwarder
	12. oc logs		: retrieve logs for specified container

LONG-LIVED or SHORT LIVED APP
	JOBS (one0time task)
	1. oc create job <job-name> --image <image> -- /bin/bash -c "date"
	2. oc create cronjob <cron-name> --image <image> --schedule="* * * * *" -- <command/date>
	3. oc create deployment <deployment-name> --image <image> <> replicas
	DEPLOYMENT
	STATEFUL SETS

POD and SERVICE NETWORK
	KUBERNETES NETWORKING
		1. Highly containet-to-container communications
		2. Pod-to-pod communications
		3. Pod-to-service commnunication
		4. External-to-service communication
	Create SVC for Deployment
	1. oc expose deployment/<deploy-name> [--selector <selector>] [--port <port>] [--target-port <target port>] [--protocol <protocol>] [--name <name>]
	Chcek SVC
	1. oc get service <svc-name> -o wide
	2. oc get endpoints
	3. oc describe deployment === finde Selector
	KUBERNETES DNS for SERVICE DISCOVERY
		Process
		1. Using DNS Operator > Deploy CoreDNS > Create svc resource > kubelet instruct pods to use CoreDNS service IP
		Each service dynamicaly assigned by Fully Qualified Domain Name (FQDN)
		SVC-NAME.PROJECT-NAME.svc.CLUSTER-DOMAIN
		SVC-NAME.PROJECT-NAME
		SVC-NAME
		Check from the container within service
		1. cat /etc/resolve.conf
	KUBERNETES NETWORKING DRIVERS
		Container Network Interface Plug-ins
		1. OVN-Kubernetes (RHOCP 4.10)
			1. oc get -n openshift-network-operator deployment/network-operator
			2. oc describe network.config/cluster
		2. Openshift SDN (RHOCP 3.x)
		3. Kuryr 
	SCALE and EXPOSE APPLICATIONS
	Service Types
		1. Cluster IP	: expose svc on cluster internal IP
		2. Load Balancer: instruct kubernetes to interact with cloud provider, then provide externally accesible IP to APP
		3. ExternalIP	: redirect traffic from a virtual IP addresses on a cluster node to pod. Cluster admin assign virtual IP to a anode, instruct RHOCP to set NAT rules
		4. NodePort	: expose svc on a port on the node IP address, redirect endpoints(pods) of the service
		5. ExternalName	: tell kubernetes that the DNS name in the externalName back the service, DNS req against Kubernetes DNS, it returns the externalName in Cannonical Name((CNAME)
	Using Routes
		Process
		1. expose HTTP and HTTPS trafic, TCP app, and non-TCP trafic by only expose HTTP and TLS-based app 
		External Request >> Ingress >> Pod >> Routes >> Service
		Resources
		1. Routes and Ingress traffic >> expose app to external network << Traffic convert by ingress from external to pods
		Create
		1. oc expose service <service name> --hostname <hostname>
		frontend-api.apps.example.com  << <route-name>-<projectname>.<default-domain>
		2. oc delete route myapp-route
		Ingress
		1.  oc create ingress ingr-sakila --rule="ingr-sakila.apps.ocp4.example.com/*=sakila-service:8080"
		 oc create ingress ingress-name --rule=URL_route=service-name:port-number
		2. oc delete ingress example-ingress
	Sticky Session (cookie)
		RHOCP auto generates the cookie for an ingress object
		create manual
		1. oc annotate ingress <ingr-name> ingress.kubernetes.io/affinity=cookie
		2. oc annotate route <route-name> router.openshift.io/cookie_name=myapp
		capture hostname
		3. ROUTE_NAME=$(oc get route <route_name> -o jsonpath='{.spec.host}')
		save and access routes
		4. curl $ROUTE_NAME -k -c /tmp/cookie_jar
		to connect again
		5. curl $ROUTE_NAME -k -b /tmp/cookie_jar
		

		

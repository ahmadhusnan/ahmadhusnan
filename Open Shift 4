OPENSHIFT
=========
COMPONENT BASED ON KUBERNETES
	kube-proxy			= maintain network between kubernetes resources
	kube-controller-manager		= Governs the state of the cluster
	kube-scheduler			= Allocate pods
	etcd				= Stores cluster data
	kube-apiserver			= Validates and configures data for the API objects
	kubelet				= Read container manifest (makesure container started and running)
	kubectl				= command to interact with kube-apiserver
	container runtime		= install podman / docker
	container-registry		= Stores and accessed container images
	pod				= smallest logical unit in kubernetes, contain one or more containers
	
SCALLING
	HORIZONTAL 			= deploy more instance
	VERTICAL			= Upgrade spek
	
FLOW
USER >>> Controller node (API server) >>> Worker Node {1..3} (Kubelet+Pods)
Contoller node			Compute Node			Storage/Registy
kube-apiserver			kubelet				PV storage
kube-scheduler			kube-proxy			Container registry
kube-controller-manager		CONTAINER RUNTIME
					Container
					Pods
----------------------------------------------------------------------------------
Physical			Virtual				Cloud

KUBERNETES RESOURCES
	Service				= expose a running application on a set of pods
	ReplicaSets			= maintain the constant pod number
	Deployment			= maintain the life cycle of an application

CLI
INSTALL OPENSHIFT CLIENT IN DESKTOP
	1. Download openshift client from redhat
		download
		- curl -LO "https://dl.k8s.io/release/$(curl -L \
  			-s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
		checksum
		- curl -LO "https://dl.k8s.io/$(curl -L \
			-s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
		check
		- echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
		install
		- sudo install -o root -g root -m 0755 kubectl \
  			/usr/local/bin/kubectl
	2. tar xvzf <FILE>
	3. echo $PATH
	4. mv <FILE> to $PATH directory
	
LOGIN
	LOGIN OCP only
	1. oc login -u user1
	LOGIN TO PROJECT
	2. oc login <optional HTTP/HTTPS/No proxy> -u user1
	3. oc cluster-info
	4. oc api-versions
	5. oc get clusteroperator
	
PROJECT
	1. oc new-project <PROJECT NAME>
	2. oc projects
	3. oc status
	
POLICY
	1. oc adm policy add-role-to-user <ROLE\Name> <NAMA_USER/IDUSER> -n <NAMESPACE>
	   for f in nama/id_user nama/id_user nama/id_user; do oc adm policy \
	   add-role-to-user admin $f -n <NAMESPACE>; done;
	2. oc get rolebindings -owide -n <NAMESPACE> | grep nama/id_usr
	
APPLICATION
	1. oc new-app <IMAGE LINK/CONTAINER LINK>
	2. oc create route edge <NAMA ROUTE> --service=<NAME SERVICE>
	3. oc get all

OPERATOR (cluster-admin)
	CLUSTER VERSION OPERATOR (CVO)
	1. oc get clusteroperator / oc get co
	2. oc describe clusteroperator <OPERATOR NAME>
	3. oc explain
	OPERATOR LIFECYCLE MANAGER (OLM)
	addon
	
VIEW PODS
	1. oc get pods -o wide -A (all namespaces)
		i. oc get pod -A --sort-by='{.metadata.creationTimestamp}' --show-labels
	2. oc logs <POD NAME>
	3. oc describe pods <POD NAME>
	4. oc get service
	5. oc get pods <PODS NAME> -o yaml
	   ex:
	   oc get pods -o yaml | yq r - 'items[0].status.podIP'  <<<< filter [index in the item array]
	   oc get pods \
	   -o custom-columns=PodName:".metadata.name",\
	   ContainerName:"spec.containers[].name",\
	   Phase:"status.phase",\
	   IP:"status.podIP",\
	   Ports:"spec.containers[].ports[].containerPort"
	6. oc explain
	7. oc adm top pods -A --sum
	8. oc adm top pods <POD NAME> -n <NAMESPACE> --containers
	9. oc logs pod-name -c <CONTAINER NAME>
	10. oc debug
	11. oc debug job/<JOB NAME> --as-user=100000
RUN PODS
	1. oc run <NAME POD> --image <IMAGENAME> --it(interaktif) --command -- <COMMAND EXECUTED> -- restart <RESTART OPTION> -- env <ENV VALUES>
	ex:
	oc run mysql --image registry.redhat.io/rhel9/mysql-80 --env MYSQL_ROOT_PASSWORD=myP@$$123
	2. oc exec <PODNAME> <OPTION> -- <COMMAND>
	ex:
	oc exec my-app -c ruby-container -- date
	oc exec my app -c ruby-container -it -- bash -il
LOGS POD
	oc logs <POD NAME> -l/--selector='' --tail= -c/--container= -f/--follow
	
DELETE PODS
	1. oc delete pod <NAME POD> -n <NAMESPACE> -l <LABEL> --grace-period=
	2. ns=<NAMESPACE>; for i in $(oc get pod -n $ns --no-header) | grep -i completed | awk '{print $1}'); do oc delete pod $i -n $ns;done
	
CLUSTER EVENTS and ALERTS
	EVENTS
	1. oc get events -n <NAMESPACE>
	2. oc describe pod <POD NAME>
	ALERTS (openshift-monitoring namespace)
	1. oc get all -n <NAMESPACE> --show-kind
	2. oc logs <POD ALERT>

CLUSTER NODE STATUS
	1. oc cluster-info
	2. oc get nodes
	   ex:
	   oc get node master01 -o json | jq '.status.conditions'
	3. oc adm node-logs --role <ROLE NAME> -u <SPECIFIED UNIT> --path=<COMMAND> --tail <BARIS>
	   ex:
	   oc adm node-logs --role master -u kubelet --path=cron --tail 1
	4. oc debug node/<NODE NAME> <<<< accest via kubelet
	5. chroot /host
	6. oc adm must-gather --dest-dir </PATH>
		tar cvaf </PATH/file must gather>
	7. oc adm inspect <SPECIFIED RESOURCES> --since <LAST TIME>
	NODE CONDITION
	CONDITION			DESCRIPTION
	OutOfDisk			If true, has insufficient free space for adding new pods
	NetworkUnavailable		If true, network not correctly configured
	NotReady			If true, one of components, such as container runtime or network is experiencing issue or not configured
	SchedullingDisabled		If true, Pods cannot be scheduled for placement on the node
	
	
SCALING THE APPLICATION
	1. oc scale --current-replicas=1 --replicas=2 deployment/<DEPLOY NAME>
	
	DEPLOY BACK-END SERVICE(Python)
		1. oc new-app python~https://github.com/openshift-roadshow/nationalparks-py.git --name nationalparks -l 'app=national-parks-app,component=nationalparks,role=backend,app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=python' --allow-missing-images=true
	CONNECTING TO A DATABASE
		1. oc new-app quay.io/centos7/mongodb-36-centos7 --name mongodb-nationalparks -e MONGODB_USER=mongodb -e MONGODB_PASSWORD=mongodb -e MONGODB_DATABASE=mongodb -e MONGODB_ADMIN_PASSWORD=mongodb -l 'app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=mongodb'
	SECRET
		1. oc create secret generic nationalparks-mongodb-parameters --from-literal=DATABASE_SERVICE_NAME=mongodb-nationalparks --from-literal=MONGODB_USER=mongodb --from-literal=MONGODB_PASSWORD=mongodb --from-literal=MONGODB_DATABASE=mongodb --from-literal=MONGODB_ADMIN_PASSWORD=mongodb
		2. oc set env --from=secret/nationalparks-mongodb-parameters deploy/nationalparks
		3. oc rollout status deployment nationalparks
		4. oc rollout status deployment mongodb-nationalparks
		
	DEPLOY MYSQL
		1. oc new-app mysql MYSQL_USER=user1 MYSQL_PASSWORD=mypa55 MYSQL_DATABASE=testdb -l db=mysql
	DEPLOY From Docker
		1. oc new-app --docker-image=docker.io/indr01/nama_image:tag --name=nama_app
	DEPLOY From Github
		1. oc new-app https://github.com/openshift/ruby-hello-world --name=ruby-hello
		
ANY UID (SERVICE ACCOUNT)
	sa = service account
	scc = security context Contraints
	1. oc create sa anyuid-nama_pod 
	     ex: oc create sa anyuid-httpd
	   oc adm policy add-scc-to-user <ANYUID> -t <ANYUID-NAME_POD>
	     ex: oc adm policy add-scc-to-user anyuid -z httpd
Perintah di atas akan memberikan role "AnyUID" kepada pod dengan nama "my-pod". Anda juga dapat menggunakan perintah oc adm policy add-scc-to-group untuk memberikan role "AnyUID" kepada sekelompok pod.

oc adm set sa dc/httpd anyuid-httpd
oc logs nama_pods // melihat logs
contoh : oc logs httpd-2-cbs29

API-RESOURCES
	1. oc api-resources
	
IMPORT-IMAGE
	1. oc import-image my-ruby
	
BUILD 
	1. oc new build </GIT_DIRECTORY>
		from build config
		1. oc start-build python

DEPLOY
	Template (YAML manifest in openshift namespace)
		1. oc process -f <templateYAML.yaml> -o yaml 
			1.1. oc process -f <templateYAML.yaml> --parameters
	CREATE DEPLOY
		1. kubectl apply -f <FILE/FILEADDR/ADDR.yaml>
			REQ FIELD
			1. apiVersion			= v1/v2/v3
			2. kind				= Deployment
			3. metadata
				1. name
				2. UID
				3. namespace
			4. spec
		2. kubectl get deployments
		3. oc new-app --template <TEMPLATE_NAME>
			3.1. oc new-app --template <TEMPLATE_NAME> --param <ENV=PARAMETER> --image <IMAGE>
			3.2. oc new-app <REPONAME>
		4. oc create deployment <DEPLOY_NAME> --image <IMAGE>
		5. oc run <POD NAME> --image=<IMAGE> --env <Env=PARAMETER> --port <PORT> 
	ROLLOUT
		1. kubectl rollout status deployment/<DEPLOY_NAME>
	REPLICASET
		1. kubectl get rs
			ex : [deployment]-[name]-[hash]
		   	      kubectl get rs nginx-deployment-979797
	LABEL
		1. kubectl get pods --show-labels
	UPDATE DEPLOYMENT
		1. kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
		2. oc set env deployment/<DEPLOY_NAME> <env=<PARAMETER>>
	DESCRIBE DEPLOYMENT
		1. kubectl describe deployments
		2. oc describe template mysql-epemeral -n openshift
	SCALING
		1. kubectl scale deployment/<DEPLOY_NAME> --replicas=10 
		if horizontal pos scalling enabled
			kubectl scale deployment/<DEPLOY_NAME> --min=10 --max=15 --cpu-percent=80
			oc scale --replicas 5 deployment/scale
	
TAG
	1. oc tag ruby:latest ruby:2.0

RESOURCE
	Types:
	1. pod (Pods)
	2. svc (Services)
	3. rs (ReplicaSet)
	4. pv (Persitent Volumes)
	5. pvc (Persistent Volume Claims)
	6. cm (ConfigMaps) and Secrets
	7. deploy (Deployment)
	8. bc (BuildConfig)
	9. dc (DeploymentConfig)
	10. routes
	11. scc (Security Context Constrains)
	12. sa (Service Account)
	every resource has .kind .apiVersion .status
	ex:
		pod.kind pod.api.Version .status
	Command:
	1. oc api-resources
	ex:
		oc api-resources --namespaced=true --api-group apps --sort-by name
	Condition Resource fields
	Field			Example				Description
	Type			ContainerReady			The Type of the condition
	Status			False				The state of the condition
	Reason			RequirementsNotMet		An optional field to provide extra info
	Message			2/3 container are running	An optional textual desc for condition
	LastTransitionTime	2023-03-07T18:05:28Z
	API Resource field
	Field			Example				Desc
	apiVersion		v1				Identifier of the object schema version
	kind			Pod				Schema Identifier
	metadata.name		<name>				Creates a label with a name key
	metadata.namespace	<namespace>			The namespace located
	metadata.labels		<label>	: app: group:		Key-value pairs identifieying metadata
	
APPLY CONFIG
	1. oc apply -f pod.json
	2. oc create -f pod.json
	3. oc delete pod/<PODNAME>
	4. oc delete pods -l app=parksmap-katacoda
	
AUTOSCALE
	1. oc autoscale deploymentconfig/<DC_NAME> --min=2 --max=5

ROLLBACK
	1. oc rollback php (--to-version=3)
	
ROLLOUT
	1. oc rollout undo deploymentconfig/<DC_NAME>
	
EDIT
	1. oc edit deploymentconfig/<PODNAME>
	2. OC_EDITOR="nano" oc edit deploymentconfig/<PODNAME>
	3. oc edit deploymentconfig/<PODNAME> -o json
	
EXPOSE (Make deployment to create service)
	1. oc expose service/<PODNAME>
	2. oc expose service/<PODNAME> --hostname=www.my-host.com
	
GET
	1. oc get pods -n default
	2. oc get nodes
	3. oc get deploymentconfig/python -o json
	
HELP
	1. oc help 
	or
	oc create --help
	2. oc explain pods
	
LOGOUT
	1. oc logout

CONTAINER IMAGES
	1. ENV
		- define the available env variable 
	2. ARG
		- define build-time variables
	3. USER
		- define the active user in container (to prevent using root)
	4. ENTRYPOINT
		- define the executeable to run when container started
	5. CMD
		- define command when start based on ENTRYPOINT
	6. WORKDIR
		- set of current working dir within container
	7. METADATA
	   EXPOSE
		- define where to store data outside container
	   VOLUME
		- where to store data outside the container
	   LABEL
		- add a key-value pair

TROUBLESHOOTING TOOLS OCP
	1. kubectl describe 	: describe resources
	2. kubectl edit 	: edit config resources
	3. kubectl patch	: update specific attribute or field for a resources
	4. kubectl replace	: deploy a new instance of the resource
	5. kubectl cp		: copy from or to container
	6. kubectl exec		: execute command within a specified container
	7. oc status		: display status container
	8. oc explain		: display documentation
	9. oc rsync		: syncronize files and dir
	10. oc rsh		: start remote shell within a specified container
	11. oc port-forward	: configure port forwarder
	12. oc logs		: retrieve logs for specified container

LONG-LIVED or SHORT LIVED APP
	JOBS (one0time task)
	1. oc create job <JOB-NAME> --image <IMAGE> -- /bin/bash -c "date"
	2. oc create cronjob <CRON-NAME> --image <IMAGE> --schedule="* * * * *" -- <COMMAND/date>
	3. oc create deployment <DEPLOYMENT-NAME> --image <IMAGE> --replicas
	DEPLOYMENT
	STATEFUL SETS

POD and SERVICE NETWORK
	KUBERNETES NETWORKING
		1. Highly containet-to-container communications
		2. Pod-to-pod communications
		3. Pod-to-service commnunication
		4. External-to-service communication
	Create SVC for Deployment
	1. oc expose deployment/<DEPLOY-NAME> [--selector <SELECTOR>] [--port <PORT>] [--target-port <TARGET PORT>] [--protocol <PROTOCOL>] [--name <NAME>]
	Chcek SVC
	1. oc get service <SVC-NAME> -o wide
	2. oc get endpoints
	3. oc describe deployment === finde Selector
	KUBERNETES DNS for SERVICE DISCOVERY
		Process
		1. Using DNS Operator > Deploy CoreDNS > Create svc resource > kubelet instruct pods to use CoreDNS service IP
		Each service dynamicaly assigned by Fully Qualified Domain Name (FQDN)
		SVC-NAME.PROJECT-NAME.svc.CLUSTER-DOMAIN
		SVC-NAME.PROJECT-NAME
		SVC-NAME
		Check from the container within service
		1. cat /etc/resolve.conf
	KUBERNETES NETWORKING DRIVERS
		Container Network Interface Plug-ins
		1. OVN-Kubernetes (RHOCP 4.10)
			1. oc get -n openshift-network-operator deployment/network-operator
			2. oc describe network.config/cluster
		2. Openshift SDN (RHOCP 3.x)
		3. Kuryr 
	SCALE and EXPOSE APPLICATIONS
	Service Types
		1. Cluster IP	: expose svc on cluster internal IP
		2. Load Balancer: instruct kubernetes to interact with cloud provider, then provide externally accesible IP to APP
		3. ExternalIP	: redirect traffic from a virtual IP addresses on a cluster node to pod. Cluster admin assign virtual IP to a a node, instruct RHOCP to set NAT rules
		4. NodePort	: expose svc on a port on the node IP address, redirect endpoints(pods) of the service
		5. ExternalName	: tell kubernetes that the DNS name in the externalName back the service, DNS req against Kubernetes DNS, it returns the externalName in Cannonical Name((CNAME)
	Using Routes
		Process
		1. expose HTTP and HTTPS trafic, TCP app, and non-TCP trafic by only expose HTTP and TLS-based app 
		External Request >> Ingress >> Pod >> Routes >> Service
		Resources
		1. Routes and Ingress traffic >> expose app to external network << Traffic convert by ingress from external to pods
		Create
		1. oc expose service <service name> --hostname <hostname>
		frontend-api.apps.example.com  << <ROUTE-NAME>-<PROJECTNAME>.<DEFAULT-DOMAIN>
		2. oc delete route myapp-route
		Ingress
		1.  oc create ingress ingr-sakila --rule="ingr-sakila.apps.ocp4.example.com/*=sakila-service:8080"
		 oc create ingress ingress-name --rule=URL_route=service-name:port-number
		2. oc delete ingress example-ingress
	Sticky Session (cookie)
		RHOCP auto generates the cookie for an ingress object
		create manual
		1. oc annotate ingress <INGR-NAME> ingress.kubernetes.io/affinity=cookie
		2. oc annotate route <INGR-NAME> router.openshift.io/cookie_name=myapp
		capture hostname
		3. ROUTE_NAME=$(oc get route <ROUTE_NAME> -o jsonpath='{.spec.host}')
		save and access routes
		4. curl $ROUTE_NAME -k -c /tmp/cookie_jar
		to connect again
		5. curl $ROUTE_NAME -k -b /tmp/cookie_jar

	MANAGE STORAGE for APP CONFIG and DATA
		1. CONFIGMAP
		provide ways to inject config data into containers (Not need protection)
		2. SECRETS
		store sensitive information (password, sesitive config files, cred, ssh key, auth token, tls certificate)
		encoded Base64
		not encrypted
		ex: echo bXl1c2VyCg== | base64 --decode
			2.1.  oc create secret generic <SECRET_NAME> --from-literal <KEY1=<SECRET1>> --from-literal <KEY2=<SECRET2>>
			2.2. kubectl create secret generic ssh-keys --from-file id_rsa=/path-to/id_rsa --from-file id_rsa.pub=/path-to/id_rsa.pub
			2.3. kubectl create configmap <CONFIG_NAME> --from-literal <key1=<config1>> --from-literal <key2=<config2>>
			2.4. oc create secret generic <SECRET_NAME> --from-file user=/tmp/demo/user --from-file root_password=/tmp/demo/root_password
			2.5 oc set volume deployment/demo --add --type secret <TYPE_SECRET> --secret-name <SECRET_NAME> --mount-path </MOUNTDIR> 
			2.6  oc delete configmap/<CONFIGMAP_NAME> -n <NAMESPACES>

	Persistent Data Volumes
		1. Method :
		- Static 	: create PV manual for cluster
		- Dynamic	: uses storage classes to create the PV on demand
		2. Storage Volume types :
		- configMap 	: app configuration in external places
		- emptyDir	: per-pod directory for scratch data
		- hostPath	: volume mount from host node to pod (must run as previlaged)
		- iSCSI		: Internet Small Computer System Interface is IP based provide block-level access to storage devices
		- local		
		- NFS		: can accessed by multiple pod at same times
		3. Persistent Volume Claims(PVC)
		- tied with namespaced not pods
		- declare what an app needs, which Kubernetes porvides on a best-effort basis
			Create PVC > update in deployment
			3.1. oc set volumes deployment/<DEPLOY_NAME> \ 1
				--add \ 2
				--name <NAME_PV> \ 3
				--type <persistentVolumeClaim> \ 4
				--claim-mode <rwo> \ 5
				--claim-size <15Gi> \ 6
				--mount-path </var/lib/example-app> \ 7
				--claim-name <example-pv-claim> 
			3.2. YAML (kind: PersistentVolumeClaim)
		4. Storage Class (Policy set)
		Manual Reclaim
			4.1 oc delete pv <PV_NAME>
			4.2 oc create pv <using data from previous PV>
			4.3 Remove data on the storage asset, and delete the storage asset
		5. Network-Attached Storage(NAS)
		File based storage architecture using two protocol IP or TCP
		- single access point
		- built in security 
		- fault-tolerance
		6. Stateful Sets87

	HIGH AVAILABILITY APPLICATION
	1. Restart Pods (configuring restart policy on pod)
		1.1. Compute Resource Requests
		- Request Request 	: Specify minimum required compute resources necessary to schedule a pod
			oc set resources deployment <DEPLOY_NAME> --requests <cpu=10m,memory=1gi>
		- Inspecting Cluster Compute Resourcess
			oc describe node <NODE_NAME>
			oc adm top node <CLUSTER_NAME>
			oc adm top pods -n <NAMESPACE>
		- Memory Limit		: If achieved, the process in the container killed by triggered OOM killer 
			oc get pod <podname> -o yaml
		- CPU Limit		: if achieved, inhibit the container, slow down the pace(CPU pressured)
	2. Probes (Check when application cannot respond to request)
		2.1. Probe Endpoints
		Function :
		- Crash mitigation by automatically attempting to restart failing pods
		- Failover and load balancing by sending requesst only to healthy pods
		- Monitoring when pods are failing
		- Scalling by determining when a new replica is ready
		Probe Types :
		- Readines Probes	: determine requests to accept or prevent by removing IP
		- Liveness Probes	: called throughtout the lifetime of the application by restarting or recreatign the pod (restart policy)
		- Startup Probes
		Tests :
		- HTTP GET 		: Probe > req to specified HTTP endpoint (success if code 200 or 399)
		- Container Command 	: Command test > Oode 0 Success
		- TCP Socket		: Cluster Open Socket to container , if connection is established
		Parameter :
		- Timing
	3. Horizontal Scaling (When load change, replicas match the load)
		- Resources Types 	: HorizontalPod Autoscaler
		- Work Flow		: Retrieves the details metric for scalling HPA > Auto scaller collect metric from metric subsystem > computes the usage percentage > computes average accross all targeted pods > get ratio
			oc autoscale deployment/hello --min 1 --max 10 --cpu-percent 80
			oc get hpa

	PRODUCTION (multi tenancy and security, operator)
	Imperative 	= command based
	Declarative	= manifest based JSON & YAML
	1. Workflow
		kubectl create -f <manifest.yml/url yml> --recursive=true/-R --save-config
		kubectl apply -f <manifest.yml/url yml> *after --save-config
	2. YAML Validation (Testing)
		--dry-run=server	= submit server-side request without persisting the resources
		--validate=true		= validate the input
	3. Comparing Resources
		kubectl diff -f <manifest.yml>
	4. Update Consideration
		not every change restart the pod, secrets and configmaps only happen in startup
		oc delete pod <pod name>
		oc rollout restart deploymeng <deployment name>
	5. 

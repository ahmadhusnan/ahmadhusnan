OPENSHIFT
=========
COMPONENT BASED ON KUBERNETES
	kube-proxy			= maintain network between kubernetes resources
	kube-controller-manager		= Governs the state of the cluster
	kube-scheduler			= Allocate pods
	etcd				= Stores cluster data
	kube-apiserver			= Validates and configures data for the API objects
	kubelet				= Read container manifest (makesure container started and running)
	kubectl				= command to interact with kube-apiserver
	container runtime		= install podman / docker
	container-registry		= Stores and accessed container images
	pod				= smallest logical unit in kubernetes, contain one or more containers
	kubeconfig			= contains cluster, users, and context

SCALLING
	HORIZONTAL 			= deploy more instance
	VERTICAL			= Upgrade spek

FLOW
USER >>> Controller node (API server) >>> Worker Node {1..3} (Kubelet+Pods)
Contoller node			Compute Node			Storage/Registy
kube-apiserver			kubelet				PV storage
kube-scheduler			kube-proxy			Container registry
kube-controller-manager		CONTAINER RUNTIME
					Container
					Pods
----------------------------------------------------------------------------------
Physical			Virtual				Cloud

KUBERNETES RESOURCES
	Service				= expose a running application on a set of pods
	ReplicaSets			= maintain the constant pod number
	Deployment			= maintain the life cycle of an application

KUBERNETES VS OPENSHIFT
Kubernetes					OpenShift
Namespaces					Projects
----------					---------
Provides a scoping mechanism for resources.	Supports increased permissions control, creation requests, 
						and default templates for improved multitenancy workflows.
						Adds additional display name and description fields.
Ingresses 					Routes
---------					------
In a plain Kubernetes cluster, using ingress	Supports TLS termination and re-encryption.
resources requires installation of an ingress 	Supports HAProxy annotation for extended features,
controller. Different ingress controllers will 	such as enforcing HTTPS, selecting a load balancing algorithm,
provide different features.			and enabling rate limiting.
Deployments 					DeploymentConfigs
-----------					-----------------
Emphasizes availability over consistency.	Emphasizes consistency over availability.
Uses ReplicaSets that support set-based 	Supports automatic rollbacks.
match selectors.				Changes to the pod template automatically trigger rollouts.
Supports pausing rollouts.			Supports custom deployment strategies and lifecycle hooks.
Red Hat recommends using Deployments unless 
you need a specific DeploymentConfigs feature.
Kustomize 					Templates
---------					---------
Kubernetes kustomize manages configurations 	OpenShift Template resources are parameterized specification resources.
using file overlays without templates.		Create resources from templates using the oc process command or the web console.
Typically, manifests and kustomize overlays 
are stored in version control.

CLI
INSTALL OPENSHIFT CLIENT IN DESKTOP
	1. Download openshift client from redhat
		download
		- curl -LO "https://dl.k8s.io/release/$(curl -L \
  			-s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
		checksum
		- curl -LO "https://dl.k8s.io/$(curl -L \
			-s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
		check
		- echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
		install
		- sudo install -o root -g root -m 0755 kubectl \
  			/usr/local/bin/kubectl
	2. tar xvzf <FILE>
	3. echo $PATH
	4. mv <FILE> to $PATH directory
	
LOGIN
	LOGIN OCP only
	1. oc login -u user1
	LOGIN TO PROJECT
	2. oc login <optional HTTP/HTTPS/No proxy> -u user1
	3. oc cluster-info
	4. oc api-versions
	5. oc get clusteroperator
	6. oc config get-contexts
	7. oc config use-context
	8. oc config set-context <name-context> <*--namespace=namespace*>
	
PROJECT
	1. oc new-project <PROJECT NAME>
	2. oc projects
	3. oc status
	
POLICY
	1. oc adm policy add-role-to-user <ROLE\Name> <NAMA_USER/IDUSER> -n <NAMESPACE>
	   for f in nama/id_user nama/id_user nama/id_user; do oc adm policy \
	   add-role-to-user admin $f -n <NAMESPACE>; done;
	2. oc get rolebindings -owide -n <NAMESPACE> | grep nama/id_usr
	
APPLICATION
	1. Run app
	- oc new-app <IMAGE LINK/CONTAINER LINK>
	- oc create route edge <NAMA ROUTE> --service=<NAME SERVICE>
	- oc get all	
	2. Security Context Constrainys (SCCs)
	- Security mechanism that limits the access from a running pod in OpenShift to the host environment. 
	  SCCs control the following host resources:
		Running privileged containers
		Requesting extra capabilities for a container
		Using host directories as volumes
		Changing the SELinux context of a container
		Changing the user ID
	- command
		oc get scc (anyuid, hostaccess, hostmount-anyuid, hostnetwork, node-exporter, nonroot, previledged, restricted)
		oc describe scc
		oc get pod podname -o yaml | oc adm policy scc-subject-review -f -
		oc create serviceaccount <service account name> -n <namespaced>
		oc adm policy add-scc-to-user SCC -z <service-account name>
		change config
		oc set serviceaccount deployment/deployment-name <service-account-name>
		note*
		Restricted :
		mostly pod created vy openshift is restricted, 
		can't download images from public registry
		pod run using random userid, cannot run pod on previledged port (<1024)
		Anyuid :
		run as user to be RunAsAny = pod run as any available user ID
		
OPERATOR (cluster-admin)
	1. CLUSTER VERSION OPERATOR (CVO)
	- oc get clusteroperator / oc get co
	- oc describe clusteroperator <OPERATOR NAME>
	- oc explain
	2. OPERATOR LIFECYCLE MANAGER (OLM) and OperatorHub
	- The operator pattern is a way to implement reusable software to manage such complex workloads.
	- An operator typically defines custom resources (CRs). 
	- installed in openshift-operator
	3. Deploying Operator (Dashboard/Catalog)
	- Cluster operators
	  Cluster operators provide the platform services of OpenShift, such as the web console and the OAuth server.
	- Add-on operators
	  OpenShift includes the Operator Lifecycle Manager (OLM). 
	  The OLM helps users to install and update operators in a cluster. 
	  Operators that the OLM manages are also known as add-on operators, 
	  in contrast with cluster operators that implement platform services.
	- Other operators
	  Software providers can create software that follows the operator pattern, and then distribute the software as manifests, Helm charts, or any other software distribution mechanism.
	4. Implementing Operator
	- Uses Kubernetes API to watch instances of the CRs and to create matching workloads
		- Operator SDK 		= Go programming language and ansible, tools to package Helm Charts as operator
		- Java Operator SDK	= Java programming language, quarkus extentsion to defelop operator
	5. Operator details
		- Details
		  Displays information about the CSV.
		- YAML
		  Displays the CSV in YAML format.
		- Subscription
		  In this tab, you can change installation options, such as the update channel and update approval.
		  This tab also links to the install plans of the operator. 
		  When you configure an operator for manual updates, you approve install plans for updates in this tab.
		- Events
		  Lists events that are related to the operator. 
	6. Troubleshooting Operators
		- examine status and condition of the CSV, subscription and install plan resources
	7. Install operator in CLI
		- Locate the operator to install.
		- Review the operator and its documentation for installation options and requirements.
		- Decide the update channel to use.
		- Decide the installation mode. For most operators, you should make them available to all namespaces.
		- Decide to deploy the operator workload to an existing namespace or to a new namespace.
		- Decide whether the Operator Lifecycle Manager (OLM) applies updates automatically,
		  or requires an administrator to approve updates.
		- Create an operator group if needed for the installation mode.
		- Create a namespace for the operator workload if needed.
		- Create the operator subscription.
		- Review and test the operator installation.
	8. Delete Operator
		- oc delete sub <subscription name>
		- oc delete csv <currentCSV>
	9. OLM Resources
		- Catalog source
		Each catalog source resource references an operator repository. 
		Periodically, the OLM examines the catalog sources in the cluster and retrieves information about the operators in each source.
			oc get catalogsource -n openshift-marketplace
			oc get csv -A
		- Package manifest
		The OLM creates a package manifest for each available operator. 
		The package manifest contains the required information to install an operator, such as the available channels.
			oc get packagemanifests
			oc describe packagemanifest web-terminal -n openshift-marketplace
		- Operator group
		Operator groups define how the OLM presents operators across namespaces.
			---
			apiVersion: operators.coreos.com/v1
			kind: OperatorGroup
			metadata:
			  name: name
			  namespace: namespace 1
			spec:
			  targetNamespaces: 2
			  - namespace
		- Subscription
		Cluster administrators create subscriptions to install operators.
			---
			apiVersion: operators.coreos.com/v1alpha1
			kind: Subscription
			metadata:
			  name: web-terminal
			  namespace: openshift-operators 1
			spec:
			  channel: fast 2
			  name: web-terminal 3
			  source: do280-catalog-redhat 4
			  installPlanApproval: Manual 5
			  sourceNamespace: openshift-marketplace
		- Operator
		The OLM creates operator resources to store information about installed operators.
		- Install plan
		The OLM creates install plan resources as part of the installation and update process. 
		When requiring approvals, administrators must approve install plans.
			oc patch installplan install-pmh78 --type merge -p '{"spec":{"approved":true}}' -n openshift-file-integrity
		- Cluster service version (CSV)
		Each version of an operator has a corresponding CSV. 
		The CSV contains the information that the OLM requires to install the operator.
		- Updates Operator
			- OpenShift provides features to help to implement such policies.
			For each installed operator, you can decide whether the OLM automatically applies updates, or whether the updates require administrator approval.
			- Operator providers can create multiple channels for an operator. 
			The provider can follow different policies to push updates to each channel, so that each channel contains different versions of the operator. 
			When installing an operator, you choose the channel to follow for updates.
			- You can create custom catalogs, and decide which versions of operators to include in the catalog. 
			For example, in a multicluster environment, you configure operators to update automatically, but add only tested versions to the catalog.
	
VIEW PODS
	1. oc get pods -o wide -A (all namespaces)
		i. oc get pod -A --sort-by='{.metadata.creationTimestamp}' --show-labels
	2. oc logs <POD NAME>
	3. oc describe pods <POD NAME>
	4. oc get service
	5. oc get pods <PODS NAME> -o yaml
	   ex:
	   oc get pods -o yaml | yq r - 'items[0].status.podIP'  <<<< filter [index in the item array]
	   oc get pods \
	   -o custom-columns=PodName:".metadata.name",\
	   ContainerName:"spec.containers[].name",\
	   Phase:"status.phase",\
	   IP:"status.podIP",\
	   Ports:"spec.containers[].ports[].containerPort"
	6. oc explain
	7. oc adm top pods -A --sum
	8. oc adm top pods <POD NAME> -n <NAMESPACE> --containers
	9. oc logs pod-name -c <CONTAINER NAME>
	10. oc debug
	11. oc debug job/<JOB NAME> --as-user=100000
RUN PODS
	1. oc run <NAME POD> --image <IMAGENAME> --it(interaktif) --command -- <COMMAND EXECUTED> -- restart <RESTART OPTION> -- env <ENV VALUES>
	ex:
	oc run mysql --image registry.redhat.io/rhel9/mysql-80 --env MYSQL_ROOT_PASSWORD=myP@$$123
	2. oc exec <PODNAME> <OPTION> -- <COMMAND>
	ex:
	oc exec my-app -c ruby-container -- date
	oc exec my app -c ruby-container -it -- bash -il
LOGS POD
	oc logs <POD NAME> -l/--selector='' --tail= -c/--container= -f/--follow
	
DELETE PODS
	1. oc delete pod <NAME POD> -n <NAMESPACE> -l <LABEL> --grace-period=
	2. ns=<NAMESPACE>; for i in $(oc get pod -n $ns --no-header) | grep -i completed | awk '{print $1}'); do oc delete pod $i -n $ns;done
	
CLUSTER EVENTS and ALERTS
	EVENTS
	1. oc get events -n <NAMESPACE>
	2. oc describe pod <POD NAME>
	ALERTS (openshift-monitoring namespace)
	1. oc get all -n <NAMESPACE> --show-kind
	2. oc logs <POD ALERT>

CLUSTER NODE STATUS
	1. oc cluster-info
	2. oc get nodes
	   ex:
	   oc get node master01 -o json | jq '.status.conditions'
	3. oc adm node-logs --role <ROLE NAME> -u <SPECIFIED UNIT> --path=<COMMAND> --tail <BARIS>
	   ex:
	   oc adm node-logs --role master -u kubelet --path=cron --tail 1
	4. oc debug node/<NODE NAME> <<<< accest via kubelet
	5. chroot /host
	6. oc adm must-gather --dest-dir </PATH>
		tar cvaf </PATH/file must gather>
	7. oc adm inspect <SPECIFIED RESOURCES> --since <LAST TIME>
	NODE CONDITION
	CONDITION			DESCRIPTION
	OutOfDisk			If true, has insufficient free space for adding new pods
	NetworkUnavailable		If true, network not correctly configured
	NotReady			If true, one of components, such as container runtime or network is experiencing issue or not configured
	SchedullingDisabled		If true, Pods cannot be scheduled for placement on the node
	
	
SCALING THE APPLICATION
	1. oc scale --current-replicas=1 --replicas=2 deployment/<DEPLOY NAME>
	
	DEPLOY BACK-END SERVICE(Python)
		1. oc new-app python~https://github.com/openshift-roadshow/nationalparks-py.git --name nationalparks -l 'app=national-parks-app,component=nationalparks,role=backend,app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=python' --allow-missing-images=true
	CONNECTING TO A DATABASE
		1. oc new-app quay.io/centos7/mongodb-36-centos7 --name mongodb-nationalparks -e MONGODB_USER=mongodb -e MONGODB_PASSWORD=mongodb -e MONGODB_DATABASE=mongodb -e MONGODB_ADMIN_PASSWORD=mongodb -l 'app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=mongodb'
	SECRET
		1. oc create secret generic nationalparks-mongodb-parameters --from-literal=DATABASE_SERVICE_NAME=mongodb-nationalparks --from-literal=MONGODB_USER=mongodb --from-literal=MONGODB_PASSWORD=mongodb --from-literal=MONGODB_DATABASE=mongodb --from-literal=MONGODB_ADMIN_PASSWORD=mongodb
		2. oc set env --from=secret/nationalparks-mongodb-parameters deploy/nationalparks
		3. oc rollout status deployment nationalparks
		4. oc rollout status deployment mongodb-nationalparks
		
	DEPLOY MYSQL
		1. oc new-app mysql MYSQL_USER=user1 MYSQL_PASSWORD=mypa55 MYSQL_DATABASE=testdb -l db=mysql
	DEPLOY From Docker
		1. oc new-app --docker-image=docker.io/indr01/nama_image:tag --name=nama_app
	DEPLOY From Github
		1. oc new-app https://github.com/openshift/ruby-hello-world --name=ruby-hello
		
ANY UID (SERVICE ACCOUNT)
	sa = service account
	scc = security context Contraints
	1. oc create sa anyuid-nama_pod 
	     ex: oc create sa anyuid-httpd
	   oc adm policy add-scc-to-user <ANYUID> -t <ANYUID-NAME_POD>
	     ex: oc adm policy add-scc-to-user anyuid -z httpd
Perintah di atas akan memberikan role "AnyUID" kepada pod dengan nama "my-pod". Anda juga dapat menggunakan perintah oc adm policy add-scc-to-group untuk memberikan role "AnyUID" kepada sekelompok pod.

oc adm set sa dc/httpd anyuid-httpd
oc logs nama_pods // melihat logs
contoh : oc logs httpd-2-cbs29

API-RESOURCES
	1. spec 	= desired status
	2. status 	= current status
	3. command
		oc api-resources

IMAGE STREAM
	1. Advantages
		- Tagging images without pushing using the command line.
    		- Configuring Builds and Deployments for automatic redeployment upon image updates.
		- Fine-grained access control and image sharing across teams.
		- Improving security by configuring view and use permissions on the image stream object.
		- Avoiding application downtime that can occur when using untested image versions.
		- Users without permissions on the source images can interact with image streams.
	2. Annotating (JSON image.openshift.io/triggers)
		- oc set trigger deploy/<DEPLOYMENT_NAME> --from-image <IMAGE_STREAM_TAG> -c <CONTAINER_NAME>.
IMPORT-IMAGE
	1. oc import-image my-ruby
	--check images update periodically
	2. oc import-image <Image> --confirm --scheduled
BUILD 
	1. oc new build </GIT_DIRECTORY>
		from build config
		1. oc start-build python

DEPLOY
	Template (YAML manifest in openshift namespace)
		1. oc process -f <templateYAML.yaml> -o yaml 
			1.1. oc process -f <templateYAML.yaml> --parameters
		2. oc get template -n openshift
		3. oc process --parameters <template name> -n openshift
		3. oc new-app --template=<template name>
		4. oc porcess -f <template name> > <name manifest.yaml>
		5. oc process <name yaml> --param-file=<param.env> > <manifest.yaml>
		6. oc process <name> --param-file=<param.env> | oc apply -f .
		7. oc create -f (update template to specific project)
		
	CREATE DEPLOY
		1. kubectl apply -f <FILE/FILEADDR/ADDR.yaml>
			REQ FIELD
			1. apiVersion			= v1/v2/v3
			2. kind				= Deployment
			3. metadata
				1. name
				2. UID
				3. namespace
			4. spec
		2. kubectl get deployments
			filtering using filter
				oc get deployment -n openshift-cluster-storage-operator --show-labels
				oc get deployment -n openshift-cluster-storage-operator -l app=csi-snapshot-controller-operator -o name
		3. oc new-app --template <TEMPLATE_NAME>
			3.1. oc new-app --template <TEMPLATE_NAME> --param <ENV=PARAMETER> --image <IMAGE>
			3.2. oc new-app <REPONAME>
			filtering jsonpath
				iterate list [*]= oc get deployment -n openshift-cluster-samples-operator cluster-samples-operator -o jsonpath='{.status.conditions[*].type}'
				index notation [0]
				condition	= jsonpath='{.status.conditions[?(@.type=="Available")].status}'
				range iteration = oc get pods -A -o jsonpath='{range .items[*]}' '{.metadata.namespace} {.metadata.creationTimestamp}{"\n"}'
			create jsonpath for report
				oc get nodes -o jsonpath-file=not_ready_nodes.jsonpath
				---jsonpath
				{range .items[*]}
				  {.metadata.name}
				  {range .status.conditions[?(@.status=="False")]}
				    {.type}{"="}{.status} {.message}
				  {end}
				{end}
		4. oc create deployment <DEPLOY_NAME> --image <IMAGE>
		5. oc run <POD NAME> --image=<IMAGE> --env <Env=PARAMETER> --port <PORT> 
		6. Waiting on Triggered Deployment Updates
		   - update secret
			deployment_generation=$(oc get -n openshift-authentication deployment/oauth-openshift -o jsonpath='{.status.observedGeneration}')
			- apply change
			while
			    new_generation=$(oc get \
			        -n openshift-authentication deployment/oauth-openshift \
			        -o jsonpath='{.status.observedGeneration}')
			    [ $new_generation -eq $deployment_generation ]
			do
		    		sleep 3
			done
			oc rollout status -n openshift-authentication deployment/oauth-openshift --revision=$new_generation
			while [ -n "$(oc get pod -n openshift-authentication -o \
			jsonpath='{.items[?(@.metadata.deletionTimestamp != "")].metadata.name}')" ]
			do
			    sleep 3
			done
	HELM CHART
	- Package that describes a set of Kubernetes resources that you can deploy. 
	- Helm charts define values that you can customize when deploying an application. 
	- A chart is a collection of files with a defined structure
	- instead of specifying the image for a deployment, charts can use user-provided values for the image. 
	- Helm charts can contain hooks that Helm executes at different points during installations and upgrades. 
	- Hooks can automate tasks for installations and upgrades. 
	- With hooks, Helm charts can manage more complex applications than purely manifest-based processes.
	1. Structure (Chart, Release, Versions)
		sample/
		├── Chart.yaml 1
		├── templates 2
		|   |── example.yaml
		└── values.yaml 3
	2. Command
		helm show chart <chartname>
		helm show values <chartname>
	3. Installing
		helm install <release-name> <chartreference>  --dry-run  --values values.yaml
		*note
			To install a chart, you must decide on the following parameters:
				The deployment target namespace
				The values to override
				The release name
	4. Release
		helm list --all-namespaces or -n 
	5. Upgrade
		helm upgrade
	6. Rollback
		helm history <relese name>
		helm rollback <releasename> <revision>
	7. Helm Repositories ( ~/.config/helm/repositories.yaml)
		helm repo add <name chart> <https://url.chart/>
		helm search repo

	ROLLOUT
		1. kubectl rollout status deployment/<DEPLOY_NAME>
	REPLICASET
		1. kubectl get rs
			ex : [deployment]-[name]-[hash]
		   	      kubectl get rs nginx-deployment-979797
	LABEL
		1. kubectl get pods --show-labels
	UPDATE DEPLOYMENT
		1. kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
		2. oc set env deployment/<DEPLOY_NAME> <env=<PARAMETER>>
	DESCRIBE DEPLOYMENT
		1. kubectl describe deployments
		2. oc describe template mysql-epemeral -n openshift
	SCALING
		1. kubectl scale deployment/<DEPLOY_NAME> --replicas=10 
		if horizontal pos scalling enabled
			kubectl scale deployment/<DEPLOY_NAME> --min=10 --max=15 --cpu-percent=80
			oc scale --replicas 5 deployment/scale
	
TAG
	1. oc tag ruby:latest ruby:2.0

RESOURCE
	Types:
	1. pod (Pods)
	2. svc (Services)
	3. rs (ReplicaSet)
	4. pv (Persitent Volumes)
	5. pvc (Persistent Volume Claims)
	6. cm (ConfigMaps) and Secrets
	7. deploy (Deployment)
	8. bc (BuildConfig)
	9. dc (DeploymentConfig)
	10. routes
	11. scc (Security Context Constrains)
	12. sa (Service Account)
	every resource has .kind .apiVersion .status
	ex:
		pod.kind pod.api.Version .status
	Command:
	1. oc api-resources
	ex:
		oc api-resources --namespaced=true --api-group apps --sort-by name
	Condition Resource fields
	Field			Example				Description
	Type			ContainerReady			The Type of the condition
	Status			False				The state of the condition
	Reason			RequirementsNotMet		An optional field to provide extra info
	Message			2/3 container are running	An optional textual desc for condition
	LastTransitionTime	2023-03-07T18:05:28Z
	API Resource field
	Field			Example				Desc
	apiVersion		v1				Identifier of the object schema version
	kind			Pod				Schema Identifier
	metadata.name		<name>				Creates a label with a name key
	metadata.namespace	<namespace>			The namespace located
	metadata.labels		<label>	: app: group:		Key-value pairs identifieying metadata
	
APPLY CONFIG
	1. oc apply -f pod.json
	2. oc create -f pod.json
	3. oc delete pod/<PODNAME>
	4. oc delete pods -l app=parksmap-katacoda
	
AUTOSCALE
	1. oc autoscale deploymentconfig/<DC_NAME> --min=2 --max=5

ROLLBACK
	1. oc rollback php (--to-version=3)
	
ROLLOUT
	1. oc rollout undo deploymentconfig/<DC_NAME>
	
EDIT
	1. oc edit deploymentconfig/<PODNAME>
	2. OC_EDITOR="nano" oc edit deploymentconfig/<PODNAME>
	3. oc edit deploymentconfig/<PODNAME> -o json
	
EXPOSE (Make deployment to create service)
	1. oc expose service/<PODNAME>
	2. oc expose service/<PODNAME> --hostname=www.my-host.com
	
GET
	1. oc get pods -n default
	2. oc get nodes
	3. oc get deploymentconfig/python -o json
	
HELP
	1. oc help 
	or
	oc create --help
	2. oc explain pods
	
LOGOUT
	1. oc logout

CONTAINER IMAGES
	1. ENV
		- define the available env variable 
	2. ARG
		- define build-time variables
	3. USER
		- define the active user in container (to prevent using root)
	4. ENTRYPOINT
		- define the executeable to run when container started
	5. CMD
		- define command when start based on ENTRYPOINT
	6. WORKDIR
		- set of current working dir within container
	7. METADATA
	   EXPOSE
		- define where to store data outside container
	   VOLUME
		- where to store data outside the container
	   LABEL
		- add a key-value pair

TROUBLESHOOTING TOOLS OCP
	1. kubectl describe 	: describe resources
	2. kubectl edit 	: edit config resources
	3. kubectl patch	: update specific attribute or field for a resources
	4. kubectl replace	: deploy a new instance of the resource
	5. kubectl cp		: copy from or to container
	6. kubectl exec		: execute command within a specified container
	7. oc status		: display status container
	8. oc explain		: display documentation
	9. oc rsync		: syncronize files and dir
	10. oc rsh		: start remote shell within a specified container
	11. oc port-forward	: configure port forwarder
	12. oc logs		: retrieve logs for specified container

LONG-LIVED or SHORT LIVED APP
	JOBS (one0time task)
	1. oc create job <JOB-NAME> --image <IMAGE> -- /bin/bash -c "date"
	2. oc create cronjob <CRON-NAME> --image <IMAGE> --schedule="* * * * *" -- <COMMAND/date>
	3. oc create deployment <DEPLOYMENT-NAME> --image <IMAGE> --replicas
	DEPLOYMENT
	STATEFUL SETS

POD and SERVICE NETWORK
	KUBERNETES NETWORKING
		1. Highly containet-to-container communications
		2. Pod-to-pod communications
		3. Pod-to-service commnunication
		4. External-to-service communication
	Create SVC for Deployment
	1. oc expose deployment/<DEPLOY-NAME> [--selector <SELECTOR>] [--port <PORT>] [--target-port <TARGET PORT>] [--protocol <PROTOCOL>] [--name <NAME>]
	Chcek SVC
	1. oc get service <SVC-NAME> -o wide
	2. oc get endpoints
	3. oc describe deployment === finde Selector
	KUBERNETES DNS for SERVICE DISCOVERY
		Process
		1. Using DNS Operator > Deploy CoreDNS > Create svc resource > kubelet instruct pods to use CoreDNS service IP
		Each service dynamicaly assigned by Fully Qualified Domain Name (FQDN)
		SVC-NAME.PROJECT-NAME.svc.CLUSTER-DOMAIN
		SVC-NAME.PROJECT-NAME
		SVC-NAME
		Check from the container within service
		1. cat /etc/resolve.conf
	KUBERNETES NETWORKING DRIVERS
		Container Network Interface Plug-ins
		1. OVN-Kubernetes (RHOCP 4.10)
			1. oc get -n openshift-network-operator deployment/network-operator
			2. oc describe network.config/cluster
		2. Openshift SDN (RHOCP 3.x)
		3. Kuryr 
	SCALE and EXPOSE APPLICATIONS
	Service Types
		1. Cluster IP	: expose svc on cluster internal IP
		2. Load Balancer: instruct kubernetes to interact with cloud provider, then provide externally accesible IP to APP
		3. ExternalIP	: redirect traffic from a virtual IP addresses on a cluster node to pod. Cluster admin assign virtual IP to a a node, instruct RHOCP to set NAT rules
		4. NodePort	: expose svc on a port on the node IP address, redirect endpoints(pods) of the service
		5. ExternalName	: tell kubernetes that the DNS name in the externalName back the service, DNS req against Kubernetes DNS, it returns the externalName in Cannonical Name((CNAME)
	Using Routes
		Process
		1. expose HTTP and HTTPS trafic, TCP app, and non-TCP trafic by only expose HTTP and TLS-based app 
		External Request >> Ingress >> Pod >> Routes >> Service
		Resources
		1. Routes and Ingress traffic >> expose app to external network << Traffic convert by ingress from external to pods
		Create
		1. oc expose service <service name> --hostname <hostname>
		frontend-api.apps.example.com  << <ROUTE-NAME>-<PROJECTNAME>.<DEFAULT-DOMAIN>
		2. oc delete route myapp-route
		Ingress
		1.  oc create ingress ingr-sakila --rule="ingr-sakila.apps.ocp4.example.com/*=sakila-service:8080"
		 oc create ingress ingress-name --rule=URL_route=service-name:port-number
		2. oc delete ingress example-ingress
	Sticky Session (cookie)
		RHOCP auto generates the cookie for an ingress object
		create manual
		1. oc annotate ingress <INGR-NAME> ingress.kubernetes.io/affinity=cookie
		2. oc annotate route <INGR-NAME> router.openshift.io/cookie_name=myapp
		capture hostname
		3. ROUTE_NAME=$(oc get route <ROUTE_NAME> -o jsonpath='{.spec.host}')
		save and access routes
		4. curl $ROUTE_NAME -k -c /tmp/cookie_jar
		to connect again
		5. curl $ROUTE_NAME -k -b /tmp/cookie_jar

	MANAGE STORAGE for APP CONFIG and DATA
		1. CONFIGMAP
		provide ways to inject config data into containers (Not need protection)
		2. SECRETS
		store sensitive information (password, sesitive config files, cred, ssh key, auth token, tls certificate)
		encoded Base64
		not encrypted
		ex: echo bXl1c2VyCg== | base64 --decode
			2.1.  oc create secret generic <SECRET_NAME> --from-literal <KEY1=<SECRET1>> --from-literal <KEY2=<SECRET2>>
			2.2. kubectl create secret generic ssh-keys --from-file id_rsa=/path-to/id_rsa --from-file id_rsa.pub=/path-to/id_rsa.pub
			2.3. kubectl create configmap <CONFIG_NAME> --from-literal <key1=<config1>> --from-literal <key2=<config2>>
			2.4. oc create secret generic <SECRET_NAME> --from-file user=/tmp/demo/user --from-file root_password=/tmp/demo/root_password
			2.5 oc set volume deployment/demo --add --type secret <TYPE_SECRET> --secret-name <SECRET_NAME> --mount-path </MOUNTDIR> 
			2.6  oc delete configmap/<CONFIGMAP_NAME> -n <NAMESPACES>

	Persistent Data Volumes
		1. Method :
		- Static 	: create PV manual for cluster
		- Dynamic	: uses storage classes to create the PV on demand
		2. Storage Volume types :
		- configMap 	: app configuration in external places
		- emptyDir	: per-pod directory for scratch data
		- hostPath	: volume mount from host node to pod (must run as previlaged)
		- iSCSI		: Internet Small Computer System Interface is IP based provide block-level access to storage devices
		- local		
		- NFS		: can accessed by multiple pod at same times
		3. Persistent Volume Claims(PVC)
		- tied with namespaced not pods
		- declare what an app needs, which Kubernetes porvides on a best-effort basis
			Create PVC > update in deployment
			3.1. oc set volumes deployment/<DEPLOY_NAME> \ 1
				--add \ 2
				--name <NAME_PV> \ 3
				--type <persistentVolumeClaim> \ 4
				--claim-mode <rwo> \ 5
				--claim-size <15Gi> \ 6
				--mount-path </var/lib/example-app> \ 7
				--claim-name <example-pv-claim> 
			3.2. YAML (kind: PersistentVolumeClaim)
		4. Storage Class (Policy set)
		Manual Reclaim
			4.1 oc delete pv <PV_NAME>
			4.2 oc create pv <using data from previous PV>
			4.3 Remove data on the storage asset, and delete the storage asset
		5. Network-Attached Storage(NAS)
		File based storage architecture using two protocol IP or TCP
		- single access point
		- built in security 
		- fault-tolerance
		6. Stateful Sets87

	HIGH AVAILABILITY APPLICATION
	1. Restart Pods (configuring restart policy on pod)
		1.1. Compute Resource Requests
		- Request Request 	: Specify minimum required compute resources necessary to schedule a pod
			oc set resources deployment <DEPLOY_NAME> --requests <cpu=10m,memory=1gi>
		- Inspecting Cluster Compute Resourcess
			oc describe node <NODE_NAME>
			oc adm top node <CLUSTER_NAME>
			oc adm top pods -n <NAMESPACE>
		- Memory Limit		: If achieved, the process in the container killed by triggered OOM killer 
			oc get pod <podname> -o yaml
		- CPU Limit		: if achieved, inhibit the container, slow down the pace(CPU pressured)
	2. Probes (Check when application cannot respond to request)
		2.1. Probe Endpoints
		Function :
		- Crash mitigation by automatically attempting to restart failing pods
		- Failover and load balancing by sending requesst only to healthy pods
		- Monitoring when pods are failing
		- Scalling by determining when a new replica is ready
		Probe Types :
		- Readines Probes	: determine requests to accept or prevent by removing IP
		- Liveness Probes	: called throughtout the lifetime of the application by restarting or recreatign the pod (restart policy)
		- Startup Probes
		Tests :
		- HTTP GET 		: Probe > req to specified HTTP endpoint (success if code 200 or 399)
		- Container Command 	: Command test > Oode 0 Success
		- TCP Socket		: Cluster Open Socket to container , if connection is established
		Parameter :
		- Timing
	3. Horizontal Scaling (When load change, replicas match the load)
		- Resources Types 	: HorizontalPod Autoscaler
		- Work Flow		: Retrieves the details metric for scalling HPA > Auto scaller collect metric from metric subsystem > computes the usage percentage > computes average accross all targeted pods > get ratio
			oc autoscale deployment/hello --min 1 --max 10 --cpu-percent 80
			oc get hpa

	PRODUCTION (multi tenancy and security, operator)
	Imperative 	= command based
	Declarative	= manifest based JSON & YAML
	1. Workflow
		kubectl create -f <manifest.yml/url yml> --recursive=true/-R --save-config
		kubectl apply -f <manifest.yml/url yml> *after --save-config
	2. YAML Validation (Testing)
		--dry-run=server	= submit server-side request without persisting the resources
		--validate=true		= validate the input
	3. Comparing Resources
		kubectl diff -f <manifest.yml>
	4. Update Consideration
		not every change restart the pod, secrets and configmaps only happen in startup
		oc delete pod <pod name>
		oc rollout restart deploymeng <deployment name>

	KUSTOMIZE
	Configuration management tool to make declarative changes to app configurations and components and oreserve the original YAML
	(kustomization.yaml at rooot)
	Patch customization
	1. Command
		- kubectl kustomize overlay/production
		- kubectl apply -k overlay/production
		- oc delete kuztomize overlay/production
	2. Structure Base directory
		base
		├── configmap.yaml
		├── deployment.yaml
		├── secret.yaml
		├── service.yaml
		├── route.yaml
		└── kustomization.yaml
			ex
			apiVersion: kustomize.config.k8s.io/v1beta1
			kind: Kustomization
			resources:
			- configmap.yaml
			- deployment.yaml
			- secret.yaml
			- service.yaml
			- route.yaml
	3. Overlays declarative artifacts, patches
		base
		----
		kustomization.yaml + resources
			development
			-----------
			kustomization.yaml (refers to base)
			testing
			-------
			kustomization.yaml (refers to base + pathces)
			prod
			----
			kustomization.yaml (refers to base + patch.yaml)
			ex.
			[user@host frontend-app]$ tree
			base
			├── configmap.yaml
			├── deployment.yaml
			├── secret.yaml
			├── service.yaml
			├── route.yaml
			└── kustomization.yaml
			overlay
			└── development
  			  └── kustomization.yaml
				apiVersion: kustomize.config.k8s.io/v1beta1
				kind: Kustomization
				namespace: dev-env
				bases:
				- ../../base
			└── testing
    			  └── kustomization.yaml
				apiVersion: kustomize.config.k8s.io/v1beta1
				kind: Kustomization
				namespace: test-env
				patches: 1
				- patch: |-
    					- op: replace 2
      					  path: /metadata/name
      					  value: frontend-test
  				  target: 3
    				    kind: Deployment
    				    name: frontend
				- patch: |- 4
    					- op: replace
      					path: /spec/replicas
      					value: 15
  				  target:
    					kind: Deployment
    					name: frontend
				bases: 5
				- ../../base
				commonLabels: 6
  				  env: test
			└── production
  			  ├── kustomization.yaml
  			  └── patch.yaml
				apiVersion: kustomize.config.k8s.io/v1beta1
				kind: Kustomization
				namespace: prod-env
				patches: 1
				- path: patch.yaml 2
				  target: 3
				    kind: Deployment
				    name: frontend
				  options:
				    allowNameChange: true 4
				bases: 5
				- ../../base
				commonLabels: 6
				  env: prod
					{patch.yaml reference}
					apiVersion: apps/v1
					kind: Deployment
					metadata:
					  name: frontend-prod 1
					spec:
					  replicas: 5 2
	4. Fitures
		- configMapGenerator
			configMapGenerator:
			- name: configmap-1  1
 			 files:
			    - application.properties
			- name: configmap-2  2
			  envs:
			    - configmap-2.env
			- name: configmap-3  3
			  literals:
			    - name="configmap-3"
			    - description="literal key-value pair"
		- secretGenerator
			secretGenerator:
			- name: secret-1  1
			  files:
			    - password.txt
			- name: secret-2  2
			  envs:
			    - secret-mysql.env
			- name: secret-3  3
			  literals:
			    - MYSQL_DB=mysql
			    - MYSQL_PASS=root
		- generatorOptions
			disableNameSuffixHash: true
  			labels:
 			   type: generated-disabled-suffix
			annotations:
 			   note: generated-disabled-suffix

	AUTHENTICATION AND AUTHORIZATION (HTPasswd & RBAC)
	1. Component:
		- user
		- Identity
		- Service Account
		- Group
		- Role
	2. Process :
		- Authentication (who is previlage)
		- Authorization (what is previlage) (Role-based Access Control)
	3. Auth Method :
		- OAuth access token
		- X.509 client certificates
		- REST API
			 must retrieve a bearer token from the OpenShift OAuth server, 
			 and then include this token as a header in requests to the API server.
	4. Auth Operator
		- runs an OAuth server grant access token to user but validate first
	5. Identity Providers
		- HTPasswd 	= validates user & password againts secret produced by command htpasswd
			Only a cluster administrator can change the data inside the HTPasswd secret. Regular users cannot change their own passwords.
			most production environments require a more powerful identity provider that integrates with the organization's identity management system.
				apiVersion: config.openshift.io/v1
				kind: OAuth
				metadata:
				  name: cluster
				spec:
				  identityProviders:
				  - name: my_htpasswd_provider 1
				    mappingMethod: claim 2
				    type: HTPasswd
				    htpasswd:
				      fileData:
				        name: htpasswd-secret 3
			command
				oc get oauth cluster -o yaml > oauth.yaml
				oc replace -f oauth.yaml
			htpasswd comand (install httpd-tools)
				htpasswd -c(create) -B -b(add/update) /tmp/htpasswd student redhat123
				htpasswd -D /tmp/htpasswd student *delete cred
				oc create secret generic htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config
				oc extract secret/htpasswd-secret -n openshift-config --to /tmp/ --confirm
				oc set data secret/htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config (redeploy openshift-authentication)
				watch oc get pods -n openshift-authentication
				oc get identities | grep manager
				oc delete identity my_htpasswd_provider:manager
				oc adm policy add-cluster-role-to-user cluster-admin student
		- Keystone 	= enables shared authentication with OpenStack Keystone v3 server
		- LDAP		= validates user & password against LDAPv3 server using simple bind authentication
		- Github 	= validate usernames and passwords against GitHub or the GitHub Enterprise OAuth authentication server.
		- OpenID Connect= OpenID Connect identity provider by using an Authorization Code Flow.
	6. Auth Cluster Administrator (in kubeconfig)
		- embeds an X.509 client certificate that never expires
			command
			INFO Run 'export KUBECONFIG=root/auth/kubeconfig' to manage the cluster with 'oc'.
			export KUBECONFIG=/home/user/auth/kubeconfig
			oc get nodes
			or
			oc --kubeconfig /home/user/auth/kubeconfig get nodes
		- authenticate as the kubeadmin virtual user. Successful authentication grants an OAuth access token.
			- kubeadmin in kube-system namespaces (secret)
			command
			oc get secret kubeadmin -n kube-system
			oc delete secret kubeadmin -n kube-system
	7. RBAC (Role-based access control)
		RBAC Object	Description
		---------------------------
		Rule		Allowed actions for objects or groups of objects.
		Role		Sets of rules. Users and groups can be associated with multiple roles.
		Binding		Assignment of users or groups to a role.
		Role Level	Description
		---------------------------
		Cluster role	Users or groups with this role level can manage the OpenShift cluster.
		Local role	Users or groups with this role level can manage only elements at a project level.
		1. command
			oc adm policy add-cluster-role-to-user <cluster-role> <username>
			oc policy add-role-to-user <role-name> <username> -n <project>
			oc adm policy remove-cluster-role-from-user <cluster-role> <username>
			 oc adm policy who-can delete user
		2. role
			Default roles		Description
			admin			Users with this role can manage all project resources, including granting access to other users to access the project.
			basic-user		Users with this role have read access to the project.
			cluster-admin		Users with this role have superuser access to the cluster resources. These users can perform any action on the cluster, and have full control of all projects.
			cluster-status		Users with this role can get cluster status information.
			edit			Users with this role can create, change, and delete common application resources on the project, such as services and deployments. These users cannot act on management resources such as limit ranges and quotas, and cannot manage access permissions to the project.
			self-provisioner	Users with this role can create projects. It is a cluster role, not a project role.
			view			Users with this role can view project resources, but cannot modify project resources.
		3. User Type
			- regular user
			- system user 		= System user names start with a system: prefix, such as system:admin, system:openshift-registry, and system:node:node1.example.com.
			- service accounts	= System account user names start with a system:serviceaccount:namespace: prefix, such as system:serviceaccount:default:deployer and system:serviceaccount:accounting:builder.
		4. Group Management
			oc adm groups new lead-developers
			oc adm groups add-user lead-developers user1
	8. Securing Kubernetes APIs (Authorization)
	- Role-based access control (RBAC) authorization is preconfigured in OpenShift. 
	  An application requires explicit RBAC authorization to access restricted Kubernetes APIs.
	- A service account is a Kubernetes object within a project. 
	- The service account represents the identity of an application that runs in a pod.
		- Create an application service account.
		- Grant the service account access to the Kubernetes API.
		- Assign the service account to the application pods.
	- API Use Case
		- Monitoring APP 	= need access to watch cluster resources to verify cluster health
		- Controllers		= GitOps tools, such as Argo, have controllers that watch cluster resources that are stored in a repository, and update the cluster to react to changes in that repository.
		- Operator		= Operators automate creating, configuring, and managing instances of Kubernetes-native applications.
	- ClusterRole
		- type : basic-user, cluster-reader, and view
		oc adm policy add-role-to-user cluster-role -z service-account
		oc adm policy add-cluster-role-to-user cluster-role service-account
			specify service account name in spec.serviceAccountName
		---
		apiVersion: rbac.authorization.k8s.io/v1
		kind: ClusterRole
		metadata:
		  name: secret-reader
		rules:
		- apiGroups: [""] 1
		  resources: ["secrets"] 2
		  verbs: ["get", "watch", "list"] 3
	- Accessing API Resources in the Same Namespace
	  	- you need a role or a cluster role and a service account in that namespace.
	- Accesing API Resources in a Different Namespace
		- create the role binding in the project with the resource.
			- create an app-sa service account in the project-1 project.
			- Assign the app-sa service account to your application pod.
			- Create a role binding on the project-2 project that references the app-sa service account and the secret-reader role or cluster role.
				system:serviceaccount:project:service-account
	9. REST API
	   - Method
	     	- Request a token from the OAuth server path /oauth/authorize?client_id=openshift-challenging-client&response_type=token. 
		  The server responds with a 302 Redirect to a location. Find the access_token parameter in the location query string.
		- Log in using the oc login command and inspect the kubeconfig yaml. This is normally located at ~/.kube/config. 
		  Find the token listed under your user entry.
		- Log in using the oc login command, and then run the oc proxy command to expose an API server proxy on your local machine. 
		  Any requests to the proxy server will identify as the user logged in with oc.
		- Log in using the oc login command, and then run the command oc whoami -t.
	  - API Verb
		API Verb	HTTP Request Command	Description
		Create		POST			Create a new resource.
		Get/List	GET			Retrieve a single resource or a list of resources.
		Update		PUT			Update a resource by providing a full specification.
		Patch		PATCH			Update a resource by providing a partial change described in a patch operation.
		Delete	DELETE				Delete a resource or a list of resources.
	  - Access
		curl -k --header "Authorization: Bearer qfyV5ULvv...i712kJT" -X GET https://api.example.com/api
	  - Find REST API
		curl -k --header "Authorization: Bearer qfyV5ULvv...i712kJT" -X GET https://api.example.com/oapi
	  - Filtering output
		curl -sk --header "Authorization: Bearer qfyV5ULvv...i712kJT" \
  -X GET https://api.example.com/api/v1/services | jq '.items[].metadata.name'
	MAINTENANCE TASK
	1. Kubernetes Batch API Resources
		- Job 		= executed once
			oc create job --dry-run=client -o yaml test --image=registry.access.redhat.com/ubi8/ubi:8.6 -- curl https://example.com
		- Cron Job	= scheduled task
			oc create cronjob --dry-run=client -o yaml test --image=registry.access.redhat.com/ubi8/ubi:8.6 --schedule='0 0 * * *' -- curl https://example.com
	NETWORK SECURITY (HTTPS)
	1. Secure Route (TLS Cerificate)
		- Edge Termination 	= terminate in router not to pods (router serving cert)
			client ---- encrypted ---- edge route (crt & key) ----- not encrypted ---- application
			oc create route edge --service api-frontend --hostname api.apps.acme.com --key api.key --cert api.crt  1 2
		- Passthrough		= encrypted traffic is sent straight to pod without TLS (app serving cert)
			client ------ application ----- container cert mounts (/usr/local/etc/ssl/certs)  
		- Re-encryption		= router terminates TLS with cert, and re-encrypted its connection to end point (two cert)
			client ---- encrypted ---- edge route (crt & key) ----- encrypted (Public Key Infrastructure[PKI]) ---- application
	2. Network Policies (ingress or egress rule)
		- developer previledge enough
		- using label (spec.ingress.namespaceSelector.matchLabels.network) (spec.ingress.podSelector.matchLabels.role) (port)instead IP addresses
		- connection between two pod different namespaces
		- command 
		  add label to namespaces
		  oc label namespace network-1 network=network-1
			kind: NetworkPolicy
			apiVersion: networking.k8s.io/v1
			metadata:
			  name: network-1-policy
			  namespace: network-1
			spec:
			  podSelector:  1
			    matchLabels:
			      deployment: product-catalog
			  ingress:  2
			  - from:  3
			    - namespaceSelector:
			        matchLabels:
			          network: network-2
				      podSelector:
				        matchLabels:
				          role: qa
			    ports:  4
			    - port: 8080
			      protocol: TCP
		allow access from Openshift Cluster SVC
			---
			apiVersion: networking.k8s.io/v1
			kind: NetworkPolicy
			metadata:
			  name: allow-from-openshift-ingress
			spec:
			  podSelector: {}
			  ingress:
			  - from:
			    - namespaceSelector:
			        matchLabels:
			          network.openshift.io/policy-group: ingress
			---
			apiVersion: networking.k8s.io/v1
			kind: NetworkPolicy
			metadata:
			  name: allow-from-openshift-monitoring
			spec:
			  podSelector: {}
			  ingress:
			  - from:
			    - namespaceSelector:
			        matchLabels:
			          network.openshift.io/policy-group: monitoring
	3. Interlal Traffic with TLS
		- Zero-trust environments require that a trusted certificate authority (CA) signs the certificates that are used to encrypt traffic. 
		- By referencing the CA certificate, an application can cryptographically verify the authenticity of another application with a signed certificate.
		- OpenShift provides the service-ca controller to generate and sign service certificates for internal traffic
		- To generate a certificate and key pair, apply the service.beta.openshift.io/serving-cert-secret-name=your-secret annotation to a service. 
			oc annotate service hello service.beta.openshift.io/serving-cert-secret-name=hello-secret
		  mount the secret in the application deployment. 
			spec:
			  template:
			    spec:
			      containers:
			        - name: hello
			          volumeMounts:
			            - name: hello-volume 1
 			             mountPath: /etc/pki/nginx/ 2
			      volumes:
 			       - name: hello-volume 3
			          secret:
			            defaultMode: 420 4
			            secretName: hello-secret 5
			            items:
			              - key: tls.crt 6
			                path: server.crt 7
			              - key: tls.key 8
 			                path: private/server.key 
		- Client Service Application Configuration
		  - Config Map
		    Apply the service.beta.openshift.io/inject-cabundle=true annotation to a configuration map to inject the CA bundle into the data: { service-ca.crt }
			oc annotate configmap ca-bundle service.beta.openshift.io/inject-cabundle=true
		  - API service
		    Applying the annotation to an API service injects the CA bundle into the spec.caBundle field.
		  - CRD
		    Applying the annotation to a CRD injects the CA bundle into the spec.conversion.webhook.clientConfig.caBundle field.
		  - Mutating or validating webhook
		    Applying the annotation to a mutating webhook or validating webhook injects the CA bundle into the clientConfig.caBundle field.
		- Key Rotation
		  - valid 26 months, rotate after 13 month, 13 month after is grace period (pod need restarted to inject new CA bundle)
		  - manual rotate
			oc delete secret certificate-secret (restart pod if needed)
			oc delete secret/signing-key -n openshift-service-ca
	4. Alternatives to Service Cert
	   - Service Mesh
	   - certmanager operator

	EXPOSING NON-HTTP SERVICE (LOAD BALANCER)
	- accesing app that need ip and port
	  a service that uses the 1.2.3.4 IP address runs an SSH server that listens on port 22.
	- A service resource contains the following information:
	  	- A selector that describes the pods that run the service
	  	- A list of the ports that provide the service on the pods
	1. Internal communication
	  Services of the ClusterIP type provide service access within the cluster.
	2. Exposing services externally
		Services of the NodePort and LoadBalancer types, as well as the use of the external IP feature of ClusterIP services, 
		expose services that are running in the cluster to outside the cluster.
	3. Load Balancer SVC
		- Metal LB operator
		  load balancer component that provides a load balancing service for clusters that do not run on a cloud provider
		  MetalLB operates in two modes: layer 2 and Border Gateway Protocol (BGP), with different properties and requirements.
		- Load Balancer SVC
			oc expose --type LoadBalancer
			oc get svc
			-----
			apiVersion: v1
			kind: Service
			metadata:
			  name: example-lb
			  namespace: example
			spec:
			  ports:
			  - port: 1234 1
			    protocol: TCP
			    targetPort: 1234
			  selector:
			    name: example 2
			  type: LoadBalancer 3

	MULTUS SECONDARY NETWORK
	- expose app externaly using secondary network (for security concern)
	- The Multus CNI (container network interface) plug-in helps to attach pods to custom networks.
	1. Configuring Secondary Network
		- use operators, such as the Kubernetes NMState operator or the SR-IOV (Single Root I/O Virtualization) network operator
		- The SR-IOV network operator configures SR-IOV network devices for improved bandwidth and latency on certain platforms and devices.
	2. Attaching Secondary Network
		- create a NetworkAttachmentDefinition resource
		- pod annotation
			- add the k8s.v1.cni.cncf.io/networks annotation to the pod's template
				apiVersion: apps/v1
				kind: Deployment
				metadata:
				  name: example
				  namespace: example
				spec:
 				 selector:
				    matchLabels:
				      app: example
				      name: example
				 template:
				    metadata:
				      annotations:
				        k8s.v1.cni.cncf.io/networks: example
				      labels:
				        app: example
				        name: example
 			- oc get pod example -o jsonpath='{.metadata.annotations.k8s\.v1\.cni\.cncf\.io/networks-status}'
	3. Attachment Custom Resources
	  Host device
	  Attaches a network interface to a single pod.
		apiVersion: k8s.cni.cncf.io/v1
		kind: NetworkAttachmentDefinition
		metadata:
		  name: example 1
		spec:
		  config: |-
		    {
		      "cniVersion": "0.3.1",
		      "name": "example", 2
		      "type": "host-device", 3
		      "device": "ens4",
		      "ipam": { 4
		        "type": "dhcp"
			--or--
			"ipam": {
			  "type": "static",
			  "addresses": [
			    {"address": "192.168.X.X/24"}
			  ]
			}
		      }
		    }
	  Bridge
	  Uses an existing bridge interface on the node, or configures a new bridge interface. 
	  The pods that are attached to this network can communicate with each other through the bridge, and to any other networks that are attached to the bridge.
	  IPVLAN
	  Creates an IPVLAN-based network that is attached to a network interface.
	  MACVLAN
	  Creates an MACVLAN-based network that is attached to a network interface.

	4. Network Operator Settings
	  You can also create the same network attachment by editing the cluster network operator configuration:
		apiVersion: config.openshift.io/v1
		kind: Network
		metadata:
		  name: cluster
		spec:
		...output omitted...
		  additionalNetworks:
		  - name: example 1
		    namespace: example 2
		    rawCNIConfig: |- 3
		      {
		        "cniVersion": "0.3.1",
		        "name": "example", 4
		        "type": "host-device", 5
		        "device": "ens4",
		        "ipam": { 6
		          "type": "dhcp"
		        }
 		     }
  		  type: Raw

	PATCHING KUBERNETES RESOURCES
		- oc patch deployment hello -p '{"spec":{"template":{"spec":{"resources":{"requests":{"cpu": "100m"}}}}}}'
		- oc patch deployment hello --patch-file ~/volume-mount.yaml

	PROJECT and CLUSTER QUOTAS
	-  By using Kubernetes role-based access control (RBAC), 
	   cluster administrators can allow users to create workloads on their own.
	1. Resources Limits
	   Kubernetes can limit the resources that a workload consumes. 
	   Workloads can specify an upper bound of the resources that they expect to use under normal operation.
	   resource limits prevent the workload from consuming an excessive amount of resources and impacting other workloads.
	2. Resources Requests
	   Workloads can declare their minimum required resources. 
	   Kubernetes tracks requested resources by workloads, and prevents deployments of new workloads if the cluster has insufficient resources.
		apiVersion: v1
		kind: ResourceQuota
		metadata:
		  name: memory
		  namespace: example
		spec:
		  hard: 1
		    limits.memory: 4Gi
		    requests.memory: 2Gi
		  scopes: {} 2
		  scopeSelector: {} 3
	3. Object Count Quotas
		- create a quota that prevents the creation of more than 10 deployments in a namespace.
			oc api-resources --api-group=""  --namespaced=true
	4. Applying Project Quotas
		oc create resourcequota example --hard=count/pods=1
		oc get quota example -o yaml
		oc get quota
		- Across Multiple Projects
		oc create clusterresourcequota example --project-label-selector=group=dev --hard=requests.cpu=10
			---
			apiVersion: quota.openshift.io/v1
			kind: ClusterResourceQuota
			metadata:
			  name: example
			spec:
			  quota: 1
			    hard:
			      limits.cpu: 4
			  selector: 2
			    annotations: {}
			    labels:
			      matchLabels:
 			       kubernetes.io/metadata.name: example
			- AppliedClusterResourceQuota
			  oc describe AppliedClusterResourceQuota -n example-2
	5. Troubleshooting Resource Quotas
		oc create resourcequota example --hard=count/deployment=1
		oc get resourcequota
		oc get event --sort-by .metadata.creationTimestamp

	MANAGING NAMESPACE RESOURCES
	1. Limit Ranges
		Default limit
		Use the default key to specify default limits for workloads.
		Default request
		Use the defaultRequest key to specify default requests for workloads.
		Maximum
		Use the max key to specify the maximum value of both requests and limits.
		Minimum
		Use the min key to specify the minimum value of both requests and limits.
		Limit-to-request ratio
		The maxLimitRequestRatio key controls the relationship between limits and requests. If you set a ratio of two, then the resource limit cannot be more than twice the request.
			 oc set resources deployment example --limits=cpu=new-cpu-limit
			---
			apiVersion: v1
			kind: ResourceQuota
			metadata:
			  name: example
			  namespace: example
			spec:
			  hard:
			    limits.cpu: "8"
			    limits.memory: 8Gi
			    requests.cpu: "4"
			    requests.memory: 4Gi

	PROJECT CREATION (Self-Provisioner Role)
	1. Resources = ProjectRequest
	2. Project Template
		- Roles and Role Bindings
		  grant specific permissions in new projects. 
		- Resource quotas and limit ranges
		- Network policies
		  Add network policies to the template to enforce organizational network isolation requirements.
	3. command
		oc adm create-bootstrap-project-template -o yaml > file (initial template)
			To define a project template and to reduce the risk of errors, you can perform the following steps:
				Create a namespace.
				Create your chosen resources and test until you get the intended behavior.
				List the resources in YAML format.
				Edit the resource listing to ensure that the definitions create the correct resources. 
					For example, remove elements that do not apply to resource creation, such as the creationTimestamp or status keys.
				Replace the namespace name with the ${PROJECT_NAME} value.
				Add the list of resources to the project template that the oc adm create-bootstrap-project-template command generates.	
		oc create -f template -n openshift-config
			--- Update the projects.config.openshift.io/cluster
			apiVersion: config.openshift.io/v1
			kind: Project
			metadata:
			...output omitted...
			  name: cluster
			...output omitted...
			spec:
			  projectRequestTemplate:
			    name: project-request
		oc describe clusterrolebinding.rbac self-provisioners
		---- To make changes, disable automatic updates with the annotation, and edit the subjects in the binding.
		oc annotate clusterrolebinding/self-provisioners --overwrite rbac.authorization.kubernetes.io/autoupdate=false
		oc patch clusterrolebinding.rbac self-provisioners -p '{"subjects": null}'

	DETECT DEPRECATED KUBERNETES API USAGE
	API version	Category	Description
	v1alpha1	Alpha	Experimental features
	v1beta1		Beta	Pre-release features
	v1		Stable	Stable features, generally available
	1. command
		---check
		oc api-resources | egrep '^NAME|cronjobs'
		egrep 'kind|apiVersion' cronjob-beta.yaml
		oc get apirequestcounts | awk '{if(NF==4){print $0}}'
		can't deploy beta but there is stable version (deprecated)
	2. Latest Kubernetes API
		Resource		Removed API Group	Current API Group
		CronJob			batch/​v1beta1		batch/​v1
		Event			events.k8s.io/​v1beta1	events.k8s.io/​v1
		PodSecurityPolicy	policy/​v1beta1		The PodSecurityPolicy admission controller will be removed
		RuntimeClass		node.​k8s.​io/​v1beta1	node.​k8s.​io/​v1
	3. Deprecated and Removed Features in Openshift
		OpenShift 4.10		OpenShift 4.11		OpenShift 4.12	Feature
		General Availability	General Availability	Deprecated	CoreDNS wildcard queries for the cluster.local domain name
		Deprecated		Deprecated		Deprecated	SQLite database format for operator catalogs
		General Availability	Removed			Removed		Removal of Jenkins images from install payload
		General Availability	Removed			Removed		Automatic generation of service account token secrets
		Deprecated		Removed			Removed		Access to Prometheus and Grafana UIs in the monitoring stack


	UPDATE OPENSHIFT
	1. Over-the-Air updates (OTA)
		- manages = controller manifests, cluster roles, and any other resources to update a cluster to a particular version.
	2. Before action
		oc get machinehealthcheck -n openshift-machine-api
		oc annotate machinehealthcheck -n openshift-machine-api machine-api-termination-handler cluster.x-k8s.io/paused=""
	3. After action
		oc annotate machinehealthcheck -n openshift-machine-api machine-api-termination-handler cluster.x-k8s.io/paused-
	4. Update Process
		- Machine Config Operator
		  The Machine Config Operator applies the desired machine state to each of the nodes.
		  This component also handles the rolling upgrade of nodes in the cluster, and uses CoreOS Ignition as the configuration format.
		- Operator Lifecycle Manager
		  The OLM orchestrates updates to any operators that are running in the cluster.
	5. Update via web console
		Administration > Cluster Settings > Update now
	6. Update via CLI
		*Be sure to update all operators that are installed through the OLM to the latest version before updating the OpenShift cluster.
		*Retrieve the cluster version and review the current update channel information. If you are running the cluster in production, then ensure that the channel reads stable.
			oc get clusterversion
			oc get clusterversion -o jsonpath='{.items[0].spec.channel}{"\n"}'
			oc patch configmap admin-acks -n openshift-config --type=merge --patch '{"data":{"ack-4.11-kube-1.25-api-removals-in-4.12":"true"}}'
			--check upgrade
			oc adm upgrade
			--apply upgarde
			oc adm upgrade --to-latest=true or oc adm upgrade --to=VERSION
			--check process
			oc get clusterversion
			oc describe clusterversion
	7. Proccess
		The updates use the rpm-ostree technology for managing transactional upgrades. 
		Updates are delivered via container images and are part of the OpenShift update process. 
		When the update deploys, the nodes pull the new image, extract it, write the packages to the disk, and then modify the bootloader to boot into the new version. 
		The machine reboots and implements a rolling update to ensure that the cluster capacity is minimally impacted.


	Continous Integration (CI) and Continous Deployment (CD)
	1. Continous Integration
		- Build Software
			building a software piece from its source code and any required dependencies, such as programming libraries
		- Run Unit Tests
		- Save Artifact
			distributable piece of software, such as a native executable file, an RPM package, a Java library archive (JAR file), or a container image.
	2. Continous Deployment
		- Find/Create Target Environtment (Integration Testing)
		- Deploy Artifact
		- Smoke Test Artifact
	3. GitOps
		GitOps is a process that stores configurations of a target environment, such as a server or a cluster, in files managed by a version control system. 
		It works under the assumption that system administrators do not change configurations on a live system directly. 
		The best practice is to change the configurations under version control, and optionally have a fellow system administrator review them, before applying to a live system.
	4. Jenkins
		Jenkins is a Java application that can be deployed to Servlet containers.
		- Describing Essential Jenkins Concepts
		  Project (or Job)
			A script that describes a workflow that Jenkins should perform, such as building an application.
		  Pipeline
    			A kind of Job that follows the pipeline concept and syntax and describes a workflow as a sequence of steps that are grouped in stages.
		  Build
			A single execution of a project, including its runtime logs and output artifacts. 
			This term comes from Jenkins' origins as a software build server.
		  Node
			A server or container that runs builds.
		  Worker
			A thread in a master node that either runs a build or orchestrates available agents.
		  Workspace
			A file system folder, dedicated to a project and sometimes also to a node, where builds store data that is either temporary, or reused between multiple builds of the same project.
		  Credential
			A Jenkins construct that provides projects and builds with access credentials to external resources. 
			There are different credentials to store user name and password pairs, SSH keys, and other credential types.
		  Plug-in
			Almost all of Jenkins functionality is extensible by plug-ins written in Java. 
			There are many community plug-ins that support different kinds of nodes, credentials, and programming languages.
			*Note that these plug-ins may depend on software tools, such as the Java Development Kit (JDK) 
			and the Node Package Manager (NPM) on agent nodes.
		- Types of Jenkins Nodes
			Master
				Stores definitions of projects and their builds.
			Agent
				Run builds (or parts of a build) under the control of a master node.
		- Jenkinsfile
    			- The pipeline directive requires one agent and one stages directive.
			- The stages directive requires one or more stage directives.
				The following directives are allowed inside pipelines directive:
				triggers: defines conditions that fire automatic execution of builds from that pipeline.
    				options: defines general configuration settings for the pipeline, overriding most properties from the web UI, for example: timeouts for running builds and retention of build logs.
    				parameters: defines parameters that a user, or an upstream pipeline, can provide for running a build.
				environment: defines environment variables available inside a pipeline or a stage.
				agent: defines which agent nodes should execute, either all stages or a single stage of the pipeline.
				Some directives, for example agent, can occur at different levels inside a pipeline, for example, at either pipeline or stage.
			Scripted pipelines 	= start with node directive and define imperative script using Groovy
			Declarative pipeline	= start with pipeline using domain-scale-language (DSL)
